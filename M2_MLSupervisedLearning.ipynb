{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the second module of this series! In this module we will take a deeper look at what Supervised Learning is and where it can be used. Then we will explore the main use cases of supervised learning, namely regression and classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Module Overview:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction to Supervised Learning](#intro)\n",
    "2. [Training of Supervised Learning Models](#training)<br>\n",
    "    2.1 [Parameters vs Hyperparameters](#params)<br>\n",
    "    2.2 [Cost Function](#cost)<br>\n",
    "3. [Evaluation of Supervised Learning Models](#evaluation)<br>\n",
    "    3.1 [Evaluation Metrics for Regression](#reg_metrics)<br>\n",
    "    3.2 [Evaluation Metrics for Classification](#clf_metrics)<br>\n",
    "    3.3 [Overfitting and Underfitting](#over_under)\n",
    "4. [Regression Models](#regression-main)<br>\n",
    "    4.1 [Linear Regression](#lr_reg)<br>\n",
    "    4.2 [Decision Trees](#dt_reg)<br>\n",
    "    4.3 [Random Forest](#rf_reg)<br>\n",
    "5. [Classification Models](#classification-main)<br>\n",
    "    5.1 [Logistic Regression](#lr-clf)<br>\n",
    "    5.2 [Decision Trees](#dt-clf)<br>\n",
    "    5.3 [Random Forest](#rf-clf)<br>\n",
    "6. [Conclusion](#conclusion)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datasets:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module we will work with the following datasets:\n",
    "1. The preprocessed Swiss Food Composition dataset from **Module 1** for classification tasks.\n",
    "2. a preprocessed version of the [Chocolate Bar Ratings](https://www.kaggle.com/datasets/evangower/chocolate-bar-ratings) dataset containing chocolate bar ratings from all over the world.\n",
    "\n",
    "You can find the preprocessed datasets in the following directory `data/*_proc.csv`. For detailed pre-processing steps you can see the `scripts/clean_chocolate_dataset.py` and `scripts/clean_swiss_food_dataset.py`.\n",
    "\n",
    "As a quick summary, we will work with the resulting Swiss Food Composition dataset after having:\n",
    "- removed the samples and features with more than 20% missing values,\n",
    "- split the dataset into training and test sets,\n",
    "- imputed missing values using mean interpolation,\n",
    "- standardized the remaining data.\n",
    "\n",
    "While the preprocessing steps performed on the Chocolate Bar Ratings dataset include:\n",
    "- splitting the dataset into training and test sets, \n",
    "- impute missing values using mean strategy,\n",
    "- standardize numerical features, \n",
    "- transform and one-hot encode `year_reviewed` and `bean_origin`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"note\">\n",
    "    <b>One-hot encoding:</b>\n",
    "    <p>Categorical data as a sequence of characters are of no use to the mathematical models behind machine learning algorithms, so we need an efficient and effective way of representing them numerically. One-hot encoding is a way of representing categorical data in a numeric form. In case you would like to learn more about it, <a href=\"https://towardsdatascience.com/one-hot-encoding-scikit-vs-pandas-2133775567b8\">see here</a>.</p>\n",
    "</div>\n",
    "\n",
    "<style>\n",
    "    /* Styling for the note admonition */\n",
    "    .note {\n",
    "        padding: 10px;\n",
    "        border-left: 4px solid #007BFF; /* Replace this with your desired border color */\n",
    "        background-color: #f0f0f0; /* Replace this with your desired background color */\n",
    "        margin: 10px 0;\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro\"></a>\n",
    "## 1. Introduction to Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supervised learning** involves machine learning algorithms that aim to build models that learn from data and are able to predict future values. It is the same as building a mathematical equation or formula with many feature variables in order to derive the desired target variable. \n",
    "**The data points that the model uses to learn already contain the corresponding targets.** This is how the model learns to derive a connection between features and targets. \n",
    "\n",
    "There are two types of problems in the supervised setting: **regression** and **classification**. In **regression**, the output that the model learns and then tries to predict is a continuous value (e.g., a person's height; the quantity of a certain nutrient or chemical in a food). In **classification**, the output that the model learns and then tries to predict is a categorical value (a class from a finite number of classes like the type of food or if a food is spoiled). \n",
    "\n",
    "[Figure 1](#sup_lear_reg_clf) illustrates the machine learning pipeline for supervised learning. From the figure, we can see that there are three main parts in the pipeline: (1) **Pre-processing of Data**, (2) **Training of the model** and (3) **Evaluation of the model**. During pre-processing, the raw data are transformed to prepare the data to be used for training, e.g., by removing outliers and imputing missing values as shown in Module 1. The pre-processed data are split into training and test sets. During training, the model will use the training set to learn. After the model has been trained, we use the test set to evaluate the model's performance on an unseen dataset. This is a way of quantifying how well the model has learned the data. When the model is ready to be used, it is given new, unseen samples and predicts either a continuous value or a label, depending on whether the task is regression or classification.\n",
    "\n",
    "<center>\n",
    "    <a id=\"sup_learn\"></a>\n",
    "    <img src=\"images/part2_supervised/supervised_learning_pipeline.jpg\" alt=\"ML Supervised Learning\" width=\"90%\">\n",
    "    <center><figcaption><em>Figure 1: Supervised Learning</em></figcaption></center>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.1 Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of regression, the model learns from the data, with the aim of outputting a continuous value similar to what it saw during the training phase. \n",
    "<center>\n",
    "    <a id=\"regression\"></a>\n",
    "    <img src=\"images/part2_supervised/regression_illustration.jpg\" alt=\"Regression\" width=\"90%\">\n",
    "    <center><figcaption><em>Figure 2: Regression</em></figcaption></center>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Figure 2](#regression) gives an illustration of regression in a 2D dataset. In this case, we have samples that have two coordinates: x and y. y depends on x, so y = f(x). The aim of regression is to find a function f(x) that *best* approximates the corresponding values of y. *Best* is quantified through the loss metrics that we are going to see in the next section. In our example, the learned function would be <span style=\"color:blue\">a line of the form $$y=ax+b$$</span> indicated in <span style=\"color:blue\">blue</span>. \n",
    "After the function (the model) is learned from the data, we can use it to predict the value of an unseen data sample (x',y') indicated in <span style=\"color:green\">green</span>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in real life, datasets have more than 2 dimensions. In this case, a line is not sufficient. This is why a plane (3 coordinates) or a hyperplane (>3 coordinates) are then used as the regression model to predict values. This is called **multiple linear regression**, in contrast to the example shown in Figure 2, which is **simple linear regression**.\n",
    "\n",
    "Besides linear regression, which assumes a linear relationship between the variables, there are other types of regression like Decision Tree regression and Random Forest regression, which we will cover further below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In classification, the target variable that the model tries to learn from the data and later predict is a class or a category. \n",
    "\n",
    "<center>\n",
    "    <a id=\"classification\"></a>\n",
    "    <img src=\"images/part2_supervised/classification_illustration.jpg\" alt=\"Classification\" width=\"90%\">\n",
    "    <center><figcaption><em>Figure 3: Classification</em></figcaption></center>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Figure 3](#classification) shows an example of samples in a 2D coordinate system. As you can see, the samples belong to two classes: <span style=\"color:red\">red</span> or <span style=\"color:green\">green</span>. As we can see, the samples of the two classes are linearly separable. So, the model can learn a line that can perfectly separate the two classes. <span style=\"color:blue\">This line is indicated in blue</span> in our example. A new, unseen sample will be classified based on which side of the boundary it will fall. In our case, the circled sample is below the boundary line, so the model will classify it as <span style=\"color:red\">red</span>.\n",
    "\n",
    "Note that in real life scenarios, datasets have many more dimensions and classes. Also, classes may not be linearly separable. In this cases, the models will be hyperplanes or other structures that will be able to capture non-linear dependencies. In this tutorial, we will see in action Logistic Regression, Decision Tree classifiers and Random Forest classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ **CHECKPOINT:**\n",
    "- In the following scenarios, would you use regression or classification algorithms:\n",
    "    1. We have a dataset that contains student characteristics and their scores in the past 5 exams. We would like to predict their *score* in the next exam.\n",
    "    2. We have a dataset that contains the ingredient composition of some food samples. we would like to predict what *category* the food belongs to.\n",
    "    3. We have a dataset that contains different images of animals. We would like to be able to predict the *type* of animal.\n",
    "    4. We have a dataset of chemical components and sensory scores for different formulations of a food product. We would like to predict the *sensory quality* of a new formulation with a modified chemical composition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"training\"></a>\n",
    "## 2. Training of Supervised Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a  id=\"params\"></a>\n",
    "### 2.1 Parameters vs Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In machine learning, parameters and hyperparameters play different roles. The **parameters** are values that **the machine learning model learns from the data**. At the end of the learning process, the data will be described by a mathematical equation. The main goal of the learning process is to find the parameters of this mathematical equation that would best describe the data. For example, suppose that you have some points scattered in a 2D coordinate system, just like in [Figure 2](#regression). Your aim is to find the line with an equation of the form: $$y = ax + b$$ \n",
    "\n",
    "In this case, `a` and `b` are the parameters that the model can learn from the points so that the learned line represents the data in the best possible way.\n",
    "\n",
    "On the other hand, **hyperparameters control the learning process itself and how the parameters will be computed**. Hyperparameters are set by the data scientists/analysts and they are not learned by the model. You can think of them as **settings or configurations to tune the learning process**. Usually people use intuition, trial-and-error, and other, more sophisticated techniques like cross-validation to pick the right hyperparameters that would make the learning process faster and produce more accurate results (for more on cross-validation see [here](https://towardsdatascience.com/cross-validation-explained-evaluating-estimator-performance-e51e5430ff85)). Coming back to the line example, the hyperparameters will determine **how complex** the equation of the line describing the points will be.\n",
    "\n",
    "All in all, **parameters determine the model output, while hyperparameters determine how the parameters are learned**. You can read more about the distinction between parameters and hyperparameters [in this blog post](https://towardsdatascience.com/parameters-and-hyperparameters-aa609601a9ac).\n",
    "\n",
    "ðŸ’¡ **CHECKPOINT:**\n",
    "- How do parameters differ from hyperparameters? What roles do they play in the learning process?\n",
    "- What is tuned by the data analyst and what is learned by the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cost\"></a>\n",
    "### 2.2 Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **cost function**, an essential part of all machine learning algorithms, quantifies how well the model can approximate the data. It is otherwise known as loss metric, loss function or objective function. It measures, how different the predictions of the model are from the true values. \n",
    "\n",
    "The cost function is a function of the parameters of the model. For example, in regression tasks, a common cost function used is the **mean squared error** (MSE): $$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2$$ where $y_i$ is the actual value and $\\hat{y_i}$ is the predicted value for sample $i$. Referring to [Figure 2](#regression), $\\hat{y_i}=ax_i + b $ and the cost function can be written as:$$\\text{MSE}=L(a,b)=\\frac{1}{n} \\sum_{i=1}^{n} (y_i - (ax_i + b))^2$$ We need to find the values of $a$ and $b$, for which minimum $MSE$ is attained. This can be solved analytically by finding the derivative of $L$ with respect to $a$ and $b$, then equalizing them to 0 and solving them for $a$ and $b$. However, an analytic solution is not always possible. That is why, to find the optimal parameters, algorithms like Gradient Descent are used. Gradient Descent is able to find the optimal solutions by computing the predictions and loss function in multiple iterations. [Figure 4](#gradient-descent) illustrates the process. You can learn more about Gradient Descent [here](https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21). \n",
    "\n",
    "Note that MSE is not the only cost function used in regression tasks. Other cost functions include: mean error, mean absolute error, etc.\n",
    "\n",
    "<center>\n",
    "    <a id=\"gradient-descent\"></a>\n",
    "    <img src=\"images/part2_supervised/lin_reg.gif\" alt=\"Gradient Descent\" width=\"90%\">\n",
    "    <center><figcaption><em>Figure 4: Cost function in Linear Regression</em></figcaption></center>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of cost function in classification is the same, however, different cost functions are applied in classification tasks. Two commonly used cost functions are: **binary cross-entropy** and **categorical cross-entropy**. **Binary cross-entropy** is used in binary class classification problems, while the **categorical cross-entropy** is used in multi-class classification problems. Again, during training, the parameters of the model are adjusted to minimize the value of the cost function, iteratively, using algorithms like Gradient Descent. You can learn more about binary and categorical cross-entropy [here](https://gombru.github.io/2018/05/23/cross_entropy_loss/).\n",
    "\n",
    "Cost functions are not only used during training but also during model evaluation, as we will see in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluation\"></a>\n",
    "## 3. Evaluation of Supervised Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics help us in assessing how suitable a machine learning model is for a specific task. They are important for quantifying how well the models have learned from the data on which they were trained, and how well a machine learning model is suited to a particular task.\n",
    "\n",
    "There are many evaluation metrics available depending on the task (classification or regression) and the characteristics of the dataset (e.g imbalanced classes). Below we will explore some of the evaluation metrics that are commonly used for classification and regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"reg_metrics\"></a>\n",
    "### 3.1 Evaluation metrics for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean Squared Error (MSE)** and **R-squared (R2)** are two of the most frequently used evaluation metrics in regression tasks. The formula for calculating MSE is: $$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2$$\n",
    "the same as in the previous section, since MSE can be used both for evaluation of the model and as a cost function.\n",
    "\n",
    "The formula for R2 is:$$R^2 = 1 - \\frac{RSS}{TSS}$$ where:\n",
    "-  $RSS$ is **the sum of squared residuals** calculated as: $RSS = (y_i - \\hat{y_i})^2$, where $y_i$ is the target value for sample $i$ and the $\\hat{y_i}$ is the predicted value for sample $i$.\n",
    "- $TSS$ is **the total sum of squares**  calculated as: $TSS = (y_i - \\bar{y})^2$, where $y_i$ is the target value for sample $i$ and $\\bar{y}$ is the mean of the target values. \n",
    "\n",
    "R2 measures the proportion of variance in the target variable  that is explained by the model. It varies between 0 and 1. An R2 of 0 indicates that the models does not capture any variance in the target variable at all, while an R2=1 indicates that the model captures all the variance in the target variable.\n",
    "\n",
    "[Figure 5](#squared-errors) gives an illustration of the squared errors for different regression lines. The error ($y_i-\\hat{y_i}$) is considered to be the vertical distance between the y coordinate of the point and the regression line, shown in blue. The squares in the figure represent the squared errors (($y_i-\\hat{y_i})^2$). \n",
    "\n",
    "<center>\n",
    "    <a id=\"squared-errors\"></a>\n",
    "    <img src=\"images/part2_supervised/linear_regression.gif\" alt=\"Linear Regression errors\" width=\"50%\" height=\"50%\">\n",
    "    <center><figcaption><em>Figure 5: Linear Regression Squared Errors</em></figcaption></center>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"clf_metrics\"></a>\n",
    "### 3.2 Evaluation metrics for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the most frequently used evaluation metrics in classification problems are: **accuracy**, **F1-score**, **receiver operating characteristic (ROC) curve** and **precision recall (PRC) curve**. The last two metrics are graphs and we are more interested in the area under this graph. Before analyzing the metrics, it is important to know and understand these terms:\n",
    " - **True Positive Samples (TP)**: these are samples that are classified as positive by the model and at the same time they belong to the positive class.\n",
    " - **True Negative Samples (TN)**: these are samples that are classified as negative by the model and at the same time they belong to the negative class.\n",
    " - **False Positive Samples (FP)**: these are samples that are classified as positive by the model but they belong to the negative class.\n",
    " - **False Negative Samples (FN)**: these are samples that are classified as negative by the model but they belong to the positive class.\n",
    "\n",
    "\n",
    "[Figure 5](#confusion-matrix) gives an illustration of these values. All the valuation metrics we will explore are based on these four values.\n",
    "<center>\n",
    "    <a id=\"confusion-matrix\"></a>\n",
    "    <img src=\"images/part2_supervised/confusion_matrix.jpg\" alt=\"Confusion Matrix\" width=\"40%\" height=\"40%\">\n",
    "    <center><figcaption><em>Figure 5: Confusion Matrix</em></figcaption></center>\n",
    "</center>\n",
    " \n",
    "\n",
    "1. **Accuracy** - is the proportion of correctly classified samples by the model out of all samples: $Accuracy = \\frac{TP+TN}{N}$, where $N$ is the total number of samples tested.\n",
    "2. **F1-score** - is the harmonic mean of precision and recall, where:\n",
    "    - $Precision = \\frac{TP}{TP+FP}$, is the ratio between the true positive samples and the total samples classified as positive by the model and,\n",
    "    - $Recall = \\frac{TP}{TP + FN}$, is the ratio of the true positive samples and the total real positive samples. \n",
    "F1 score balances these metrics and it is helpful especially in dealing with imbalanced classes. F1 score ranges from 0 to 1, with values near 1 indicating a good model.\n",
    "3. **ROC curve** - is a graph that plots the true positive rate (precision) against the false positive rate ($FPR = \\frac{FP}{FP + TN}$) at different classification thresholds (e.g at different threshold probabilities to classify a sample as positive). When using this metric, we are more interested in the area under the ROC curve (**ROC AUC**), the higher the area under the ROC curve, the better the model. The maximum value achievable is 1. For more information on the ROC curve, you can read [this article](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc).\n",
    "4. **PRC curve** - plots the precision vs the recall across different classification thresholds, just like the ROC curve. Again the higher the value of the area under the curve (**AUC PRC**), the better the performance of the model. The maximum value achievable is 1. \n",
    "\n",
    "\n",
    "\n",
    "<div class=\"note\">\n",
    "    <b>Classification Threshold:</b>\n",
    "    <p>The classification threshold determines the minimum required score (probability) for a sample to be classified as positive. For example, if we set the classification threshold in a classification model to 80%, this means that the model will only classify as positive those samples that are predicted to be positive with a probability of 80% or greater. All other samples will be classified as negative. On the other hand, if we set the threshold to 50%, all samples that are predicted to be positive with a probability of 50% or greater will be classified as positive.</p>\n",
    "</div>\n",
    "\n",
    "<style>\n",
    "    /* Styling for the note admonition */\n",
    "    .note {\n",
    "        padding: 10px;\n",
    "        border-left: 4px solid #007BFF; /* Replace this with your desired border color */\n",
    "        background-color: #f0f0f0; /* Replace this with your desired background color */\n",
    "        margin: 10px 0;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "\n",
    "ðŸ’¡**CHECKPOINT:**\n",
    "- What do you think will happen with the number of predicted positive and negative samples if we increase the classification threshold from 50% to 80%?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"over_under\"></a>\n",
    "### 3.3 Overfitting and Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, in order to better assess the performance of the model, it is necessary to check the performance on both the training and the test sets and compare the metrics. We want a model that has optimal performance and is neither underfitting nor overfitting.\n",
    "\n",
    "**Overfitting** - happens when the performance of the model on the training set is a lot higher than on the test set. In this case, the model has learned the training set *by heart*, but cannot generalize well on new, unseen samples. In other words, the model overfits the training set. Some of the methods to deal with overfitting include: using a less complex model and regularization. \n",
    "\n",
    "**Underfitting** - happens when the model cannot learn the underlying relationships of the data because it might be too simple. The performance metrics on both the training and test sets are low. One way to deal with underfitting is to choose a more complex model.\n",
    "\n",
    "[Figure 6](#over-under-fitting) illustrates overfitting and underfitting.\n",
    "<center>\n",
    "    <a id=\"over-under-fitting\"></a>\n",
    "    <img src=\"images/part2_supervised/overfitting_underfitting.jpg\" alt=\"Overfitting Underfitting\" width=\"80%\" height=\"60%\">\n",
    "    <center><figcaption><em>Figure 6: Overfitting and Underfitting Illustration</em></figcaption></center>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"note\">\n",
    "    <b>Regularization:</b>\n",
    "    <p>It is a way to penalize complex models and thus reduce overfitting during training. Since machine learning models are trained on the training set, they try to learn the training set in the best possible way. \n",
    "    This increases the risk that the model will not generalise well to new, unseen data. Regularisation therefore helps to make the model more robust and avoid overfitting. To learn more about regularization, <a href=\"https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/video-lecture\">see here</a>.</p>\n",
    "</div>\n",
    "\n",
    "<style>\n",
    "    /* Styling for the note admonition */\n",
    "    .note {\n",
    "        padding: 10px;\n",
    "        border-left: 4px solid #007BFF; /* Replace this with your desired border color */\n",
    "        background-color: #f0f0f0; /* Replace this with your desired background color */\n",
    "        margin: 10px 0;\n",
    "    }\n",
    "</style>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ **CHECKPOINT:**\n",
    "- How does a model perform on the training or test set when it overfits? What about when it underfits?\n",
    "- *We have a dataset that contains 1000 points, each of them having a single feature. The function that will approximate these points will have the form $y=ax^2+bx+c$. We do not want the model to overfit the data, so we set a regularization parameter Î»=0.1, to be used during the training process. Since we do not want to wait long, we also determine the number of steps that the loss function will be computed and the weights updated, n=100.* Given this scenario, determine whether a, b, c, Î» and n are parameters or hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"regression-main\"></a>\n",
    "## 4. Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to cover three different regression models: Linear Regression, Decision Trees and Random Forest. After training these models we will evaluate their performance on the Chocolate Bar Ratings dataset. Note that this dataset is already pre-processed. All pre-processing steps can be found in the `scripts/clean_chocolate_dataset.py` file. \n",
    "\n",
    "First, we will start by importing the necessary libraries, reading the dataset and defining some helper functions that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as  pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import ipywidgets as widgets\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import PrecisionRecallDisplay, RocCurveDisplay\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join( \"data\", \"chocolate_bars_proc.csv\")\n",
    "chocolate_dataset = pd.read_csv(filepath, index_col='id')\n",
    "chocolate_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below code cell, we will select only the features that we will need for the regression task. Our target feature (the value the model will learn to predict) will be the `rating`. Thus, if we give as input to the model the bean origin, the year of the review, the cocoa percentage and the number of ingredients, it will be able to predict the rating of the chocolate bar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chocolate_dataset = chocolate_dataset[['cocoa_percent','num_ingredients', 'year_binary',\\\n",
    "                                           'country_venezuela', 'country_peru', \\\n",
    "                                                'split', 'rating']]\n",
    "chocolate_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_sets(dataset:pd.DataFrame, feature_cols:list, target_col:str) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    '''\n",
    "    Function used to split the dataset intro training and test sets and return the train\n",
    "    and test features and target variables. The split is based on the pre-processing steps \n",
    "    included in the clean_chocolate_dataset.py script.\n",
    "\n",
    "    Args:\n",
    "     - dataset: the dataset that will be split\n",
    "     - feature_cols: a list with the names of the feature columns\n",
    "     - target_col: a string with the name of the target column\n",
    "\n",
    "    Returns:\n",
    "     - four pandas dataframes containing the training features, training targets,\n",
    "     test features and test targets.\n",
    "    '''\n",
    "\n",
    "    train_data = dataset[dataset['split'] == 'train']\n",
    "    test_data = dataset[dataset['split'] == 'test']\n",
    "    \n",
    "    return train_data[feature_cols], train_data[target_col], \\\n",
    "        test_data[feature_cols], test_data[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_regressor(regressor, title:str, dataset:pd.DataFrame) -> None:\n",
    "    '''\n",
    "    This function will take a regressor object as well as the title\n",
    "    of the plot that will be visualized, as input. It will train the\n",
    "    regressor on the training set and get regressor's predictions on the\n",
    "    trainiing and test sets. In the end it will call the visualize_results\n",
    "    function to visualize the results that the trained model can achieve.\n",
    "\n",
    "    Args:\n",
    "     - regressor: is the regression object\n",
    "     - title: the title of the plot that will be displayed\n",
    "     - dataset: the dataset that will be learned\n",
    "\n",
    "    Returns: nothing\n",
    "    '''\n",
    "    feature_cols = dataset.columns.difference(['split', 'rating'])\n",
    "    target_col = 'rating'\n",
    "\n",
    "    train_set_features, train_set_ratings, \\\n",
    "        test_set_features, test_set_ratings = get_train_test_sets(chocolate_dataset, \n",
    "                                                                  feature_cols, \n",
    "                                                                  target_col)\n",
    "    \n",
    "    regressor.fit(train_set_features, train_set_ratings)\n",
    "    \n",
    "    y_train_pred = regressor.predict(train_set_features)\n",
    "    y_test_pred = regressor.predict(test_set_features)\n",
    "\n",
    "    visualize_results(train_set_ratings, y_train_pred, \n",
    "                      test_set_ratings, y_test_pred,\n",
    "                      title)\n",
    "\n",
    "def visualize_results(y_train_true:np.array, y_train_pred:np.array, \n",
    "                      y_test_true:np.array, y_test_pred:np.array, title:str) -> None:\n",
    "    '''\n",
    "    This function is used to visualize the results achieved by a regressor.\n",
    "\n",
    "    Args:\n",
    "     - y_train_true: the true targets of the training set\n",
    "     - y_train_pred: the predicted targets of the training set from the regressor\n",
    "     - y_test_true: the true targets of the test set\n",
    "     - y_test_pred: the predicted targets of the test set from the regressor\n",
    "     - title: the title of the plot that will be displayed\n",
    "\n",
    "    Returns: nothing\n",
    "    '''\n",
    "\n",
    "    mse_train = f\"MSE (Training) = {mean_squared_error(y_train_true, y_train_pred):.3f}\"\n",
    "    mse_test = f\"MSE (Test) = {mean_squared_error(y_test_true, y_test_pred):.3f}\"\n",
    "    r2_train = f\"R2 (Training) = {r2_score(y_train_true, y_train_pred):.3f}\"\n",
    "    r2_test = f\"R2 (Test) = {r2_score(y_test_true, y_test_pred):.3f}\"\n",
    "\n",
    "    _, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,4))\n",
    "\n",
    "    axes[0].scatter(x=y_train_true, y=y_train_pred, s=4)\n",
    "    axes[0].plot(y_train_true, y_train_true, color='red', label='Perfect Model')\n",
    "    axes[0].set_xlabel(\"True Rating\")\n",
    "    axes[0].set_ylabel(\"Predicted Rating\")\n",
    "    axes[0].set_title(\"Predicted vs Actual - Training Set\");\n",
    "    axes[0].text(3.5, 1.25, mse_train, fontsize=10, color='black', ha='center', va='center')\n",
    "    axes[0].text(3.5, 1.0, r2_train, fontsize=10, color='black', ha='center', va='center')\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].scatter(x=y_test_true, y=y_test_pred, s=4)\n",
    "    axes[1].plot(y_test_true, y_test_true, color='red', label='Perfect Model')\n",
    "    axes[1].set_xlabel(\"True Rating\")\n",
    "    axes[1].set_ylabel(\"Predicted Rating\")\n",
    "    axes[1].set_title(\"Predicted vs Actual - Test Set\");\n",
    "    axes[1].text(3.65, 1.75, mse_test, fontsize=10, color='black', ha='center', va='center')\n",
    "    axes[1].text(3.65, 1.5, r2_test, fontsize=10, color='black', ha='center', va='center')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.suptitle(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions above will help us later when we create, initialize and run the models. The first function `get_train_test_sets()` will take the original dataset as an argument and will return 4 datasets corresponding to the training features, test features, training targets and test targets. The second function, `run_regressor()` will take  a regressor object, the title that will be displayed in the plot and original dataset. It will train the regressor model on the training features and target variables and then will call the `visualize_results()` function. As its name indicates, it will calculate the MSE and R2 in both the training and test sets and will plot the `actual vs predicted ratings` for the target variable of the dataset. \n",
    "\n",
    "Now we are ready to initialize our models and see their performance on our regression task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lr_reg\"></a>\n",
    "### 4.1 Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression is one of the simplest regression models. Given the data it will try to find a linear equation that best fits it. Our model will be of the form:\n",
    "$$rating  = w_1\\times cocoa\\_percent + w_2\\times num\\_ingredients + w_3\\times year\\_binary + w_4\\times country\\_venezuela + w_5\\times country\\_peru$$\n",
    "\n",
    "During training it will learn the weights: $w_1, w_2, w_3, w_4, w_5$. After training, using the above equation it will be able to predict a rating for a given set of features. We will use the `initialize_linear_regressor` function to initialize the `LinearRegression()` object from the `sklearn` library. We determine the `title` of the plot when the results will be visualized and then, we call the `run_regressor()` function which will train the model and then call the visualization function. You can run the two below code cells to see the results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_linear_regressor(chocolate_dataset:pd.DataFrame):\n",
    "    regressor = LinearRegression()\n",
    "    title = f\"Linear Regression\"\n",
    "    run_regressor(regressor, title, chocolate_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_linear_regressor(chocolate_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots show the performance of the model both on the training and test sets. The red line indicates a perfect match between the actual and predicted ratings, while the blue dots plot the actual and predicted values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dt_reg\"></a>\n",
    "### 4.2 Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to linear regression, a decision tree will predict a rating by *asking a series of questions about the data*. [Figure 7](#decision-trees) gives a high level view of the process. The model operates by following a series of decisions from the root of the tree down to the leaves, following the direction indicated by the arrows in Figure 7. Depending on the values the features of the sample have, the tree is able to make a prediction about the rating of a sample. The thresholds for making a split as well as the nodes themselves are learned during the training process.\n",
    "<center>\n",
    "    <a id=\"decision_tree\"></a>\n",
    "    <img src=\"images/part2_supervised/decision_tree.jpg\" alt=\"Decision Tree\" width=\"60%\" height=\"60%\">\n",
    "    <center><figcaption><em>Figure 7: Decision tree model Illustration</em></figcaption></center>\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in linear regression, we use the `initialize_decision_tree_regressor()` to initialize the decision tree model and pass `max_depth` hyperparameter as a parameter to the function. One of the most important hyperparameters that we can finetune is the `max_depth`. Through it we can control how deep the tree will be. The deeper the tree the more likely that the model will learn the dataset by heart and overfit. By keeping the trees shallow, we reduce overfitting. It is important though to find a balance in order to prevent underfitting as well. When initializing the `DecisionTreeRegressor` we pass the `random_state=0` hyperparameter to make the results reproducible. The rest of the procedure is the same as in Linear Regression. This function gets the dataset as a parameter as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_decision_tree_regressor(max_depth:int, chocolate_dataset:pd.DataFrame):\n",
    "    regressor = DecisionTreeRegressor(max_depth=max_depth, random_state=0)\n",
    "    title = f\"Decision Tree Regression - max_depth={max_depth}\"\n",
    "    run_regressor(regressor, title, chocolate_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can interact with the slider below and try different values for the `max_depth` hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_slider_1 = widgets.IntSlider(value=10, min=1, max=20, step=1, description='max_depth:')\n",
    "widgets.interact_manual(initialize_decision_tree_regressor,\n",
    "                        max_depth=max_depth_slider_1,\n",
    "                        chocolate_dataset = widgets.fixed(chocolate_dataset));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ **CHECKPOINT:**\n",
    "- In the above code snippet, try different values for `max_depth`. How does increasing the value of this hyperparameter affect model performance in train and test sets? What does that mean? Which value of `max_depth` would you choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rf_reg\"></a>\n",
    "### 4.3 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest models are part of the family of ensemble learning models. Ensemble learning models combine different machine learning models with the aim of increasing the performance. In these models, a prediction is reached by aggregating the predictions of each of the models that make up the ensemble. [Figure 8](#random-forest) gives an illustration of a random forest model. The random forest model is made of many decision trees. The number of decision trees is determined by the `n_estimators` hyperparameter. Meanwhile, the depth of each decision tree is determined by the `max_depth` hyperparameter. In the below code cells, you can use the sliders to experiment with different values of these hyperparameters and see how they affect the MSE and R2 evaluation metrics.\n",
    "<center>\n",
    "    <a id=\"random_forest\"></a>\n",
    "    <img src=\"images/part2_supervised/random_forest.jpg\" alt=\"Random Forest\" width=\"80%\" height=\"60%\">\n",
    "    <center><figcaption><em>Figure 8: Random Forest Model Illustration</em></figcaption></center>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_random_forest_regressor(n_estimators:int, max_depth:int, chocolate_dataset:pd.DataFrame):\n",
    "    regressor = RandomForestRegressor(n_estimators=n_estimators, \n",
    "                                      max_depth=max_depth, random_state=0)\n",
    "    title = f\"Random Forest Regression - n_estimators={n_estimators} and max_depth={max_depth}\"\n",
    "    run_regressor(regressor, title, chocolate_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators_slider = widgets.IntSlider(value=30, min=10, max=1000, step=5, description='n_estimators:')\n",
    "max_depth_slider_2 = widgets.IntSlider(value=10, min=1, max=20, step=1, description='max_depth:')\n",
    "widgets.interact_manual(initialize_random_forest_regressor,\n",
    "                        n_estimators=n_estimators_slider,\n",
    "                        max_depth=max_depth_slider_2,\n",
    "                        chocolate_dataset = widgets.fixed(chocolate_dataset));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ **CHECKPOINT:**\n",
    "- In the above code snippet, try different values for `n_estimators` and `max_depth`. How does increasing the values of these hyperparameters affect model performance in training and test sets? What does that mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"classification-main\"></a>\n",
    "## 5. Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to cover three different classification models: Logistic Regression, Decision Trees and Random Forest. After training we will evaluate their performance on the Swiss Food Composition dataset. Note that this dataset is already pre-processed. All pre-processing steps can be found in the `scripts/clean_swiss_food_dataset.py` file. The classification task will consist of predicting the category of the food sample which will be **binary**. This transformation of the category column happens in the `prepare_dataset()` helper function.\n",
    "\n",
    "First, we will start by importing the necessary libraries, reading the dataset and defining some helper functions that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(\"data\", \"swiss_food_composition_proc.csv\")\n",
    "dataset = pd.read_csv(filepath, index_col=0)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label_category(label:str, label_column:pd.Series) -> pd.Series:\n",
    "    '''\n",
    "    This function converts the label_column to a column containing binary values\n",
    "    of 0 or 1. The rows whose value will be equal to `label` will contain a 1. All\n",
    "    the other rows will contain a 0.\n",
    "\n",
    "    Args:\n",
    "        - label: the category that will be encoded with 1\n",
    "        - label_column: the column that will be transformed\n",
    "\n",
    "    Return:\n",
    "    The transformed column.\n",
    "    '''\n",
    "    return label_column.apply(lambda x: 1 if x == label else 0)\n",
    "\n",
    "def prepare_dataset(label:str, original_dataset:pd.DataFrame) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    \n",
    "    ''' \n",
    "    This function is used to prepare the original dataset for the next steps in \n",
    "    supervised learning. It extracts the labels, does the manual encoding of the \n",
    "    category specified in the label parameter and splits the dataset into training and \n",
    "    test based on the split from Module 1. Also, here the features and labels are\n",
    "    splitted in different dataframes.\n",
    "\n",
    "    Args: \n",
    "        - label: the category that will be classified\n",
    "        - original_datatset: the dataset\n",
    "    \n",
    "    Returns:\n",
    "        - train_features, test_features, train_labels, test_labels\n",
    "    '''\n",
    "    \n",
    "    labels_col = original_dataset['category']\n",
    "    \n",
    "    one_hot_labels = encode_label_category(label=label, label_column=labels_col)\n",
    "    \n",
    "    original_dataset['category_binary'] = one_hot_labels\n",
    "    \n",
    "    not_needed_cols = ['name', 'category', 'split', 'category_binary']\n",
    "    feature_cols = original_dataset.columns.difference(not_needed_cols)\n",
    "    target_col = 'category_binary'\n",
    "\n",
    "    return get_train_test_sets(original_dataset, feature_cols=feature_cols, target_col=target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cereals_train_set_features, \\\n",
    "    cereals_train_set_labels, \\\n",
    "    cereals_test_set_features, \\\n",
    "        cereals_test_set_labels = prepare_dataset(\"cereals\", dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can have a look, what the training and test sets contain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cereals_train_set_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cereals_train_set_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot_titles(category:str, max_depth:int, n_estimators:int, clf_type:str) -> str:\n",
    "    '''\n",
    "    This function creates the title of the plot that will be displayed.\n",
    "    The title is created based on the category that will be learned,\n",
    "    the hyperparameters of the classifier and the classifier type.\n",
    "\n",
    "    Parameters:\n",
    "     - category: is the category that is learned.\n",
    "     - max_depth: determines the maximum depths of the\n",
    "     decision tree or random forest. Needed only if clf_type \n",
    "     is `dt` or `rf`.\n",
    "     - n_estimators: is the number of trees in the random forest\n",
    "     classifier. Needed only if clf_type is `rf`.\n",
    "     - clf_type: the type of classifier: logistic regression (lr),\n",
    "     decision tree (dt) and random forest (rf).\n",
    "\n",
    "     Returns:\n",
    "     The plot title.\n",
    "    '''\n",
    "\n",
    "    if clf_type==\"lr\":\n",
    "        return f'Logistic Regression: category={category}'\n",
    "    elif clf_type==\"dt\":\n",
    "        return f'Decision Tree: category={category}: max_depth={max_depth}'\n",
    "    elif clf_type==\"rf\":\n",
    "        return f'Random Forest: category={category}: max_depth={max_depth} and n_estimators={n_estimators}'\n",
    "    return ''\n",
    "\n",
    "def run_auc_roc_prc(clf, X_train:pd.DataFrame, X_test:pd.DataFrame, y_train:pd.DataFrame, y_test:pd.DataFrame, sup_title:str):\n",
    "\n",
    "    '''\n",
    "    This function is used to visualize the ROC and PRC curves, we well as to\n",
    "    calculate and display the accuracy and F1 scores. \n",
    "\n",
    "    Args:\n",
    "     - clf: the classifier\n",
    "     - X_train: is the features of the training set\n",
    "     - X_test: is the features of the test set\n",
    "     - y_train: the labels of the training set\n",
    "     - y_test: the labels of the test set\n",
    "     - sup_title: the title of the plot\n",
    "\n",
    "    Returns: nothing\n",
    "    '''\n",
    "\n",
    "    roc_title = f'ROC - curve'\n",
    "    prc_title = f'PRC - curve'\n",
    "      \n",
    "    print(f\"Accuracy (Test): {accuracy_score(y_true=y_test, y_pred=clf.predict(X_test)):.2f}\")\n",
    "    print(f\"F1-score (Test): {f1_score(y_true=y_test, y_pred=clf.predict(X_test)):.2f}\")\n",
    "    print(f\"Accuracy (Train): {accuracy_score(y_true=y_train, y_pred=clf.predict(X_train)):.2f}\")\n",
    "    print(f\"F1-score (Train): {f1_score(y_true=y_train, y_pred=clf.predict(X_train)):.2f}\")\n",
    "    \n",
    "\n",
    "    _, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "    axes[0].set_title(roc_title)\n",
    "    axes[1].set_title(prc_title) \n",
    "    RocCurveDisplay.from_estimator(clf, X_test, y_test, \\\n",
    "        name=\"Test set\", ax=axes[0], alpha=0.8, plot_chance_level=True)\n",
    "    RocCurveDisplay.from_estimator(clf, X_train, y_train, \\\n",
    "        name=\"Training set\", ax=axes[0], alpha=0.8)\n",
    "    PrecisionRecallDisplay.from_estimator(clf, X_test, y_test,\\\n",
    "         name=\"Test set\", ax=axes[1], alpha=0.8, plot_chance_level=True)\n",
    "    PrecisionRecallDisplay.from_estimator(clf, X_train, y_train,\\\n",
    "         name=\"Training set\", ax=axes[1], alpha=0.8)\n",
    "\n",
    "    plt.suptitle(sup_title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lr-clf\"></a>\n",
    "### 5.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite of its name, logistic regression is a classification model. It is the equivalent of Linear Regression in classification tasks. It tries to find a linear equation that receives the features of a sample as input and outputs the predicted class. The model's equation will be similar to that of Linear Regression:\n",
    "$$category \\;probability  = \\frac{1}{1+e^{-(w_1\\times f_1 + w_2\\times f_2 + {w_3}\\times f_3 + ... + w_n\\times f_n)}}$$\n",
    "During training, the model will learn the weights $w_1, w_2, ..., w_n$. After training, when it will receive a new sample, it will compute the above function to find the category probability and based on a classification threshold determined by the data scientist, it will predict the category.\n",
    "\n",
    "Below, the `run_logistic_regression_classifier()` function will initialize the Logistic Regression object by giving the `random_state=0` as argument to make the results reproducible. Then we call the `run_auc_roc_prc()` function to visualize the results obtained by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_logistic_regression_classifier(dataset:pd.DataFrame):\n",
    "    '''\n",
    "    This function splits the dataset into the training and test sets. Then it \n",
    "    creates the plot title. Afterwards, it initializes the model and trains it.\n",
    "    Then the results are visualized by calling the run_auc_roc_prc() helper\n",
    "    function.\n",
    "    \n",
    "    Args:\n",
    "     - dataset: a pandas dataframe containing the dataset\n",
    "    \n",
    "    Returns: nothing\n",
    "    '''\n",
    "    # split the dataset into training and test set\n",
    "    train_data_features, train_data_labels,\\\n",
    "          test_data_features, test_data_labels \\\n",
    "            = prepare_dataset(label=\"cereals\", original_dataset=dataset)\n",
    "    \n",
    "    # create the title of the plot\n",
    "    sup_title = get_plot_titles('Cereals', max_depth=None, \\\n",
    "                                n_estimators=None, clf_type=\"lr\")\n",
    "\n",
    "    # initialize the model\n",
    "    lr_classifier = LogisticRegression(random_state=0)\n",
    "    # train the model\n",
    "    lr_classifier.fit(train_data_features, train_data_labels.values.ravel())\n",
    "    # visualize results\n",
    "    run_auc_roc_prc(lr_classifier, X_train=train_data_features, \n",
    "                                   y_train=train_data_labels, X_test=test_data_features,\n",
    "                                   y_test=test_data_labels, sup_title=sup_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logistic_regression_classifier(dataset=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dt-clf\"></a>\n",
    "### 5.2 Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees, can be used for classification tasks, in the same way as they are used for regression tasks. The only difference is that instead of predicting continuous values they will predict categories. [Figure 9](#decision_tree_clf) illustrates the process. Again the split thresholds are learned during training. Just like in regression, we determine the depth of the tree using the `max_depth` hyperparameter, that has the same function as in the regression example above.\n",
    "<center>\n",
    "    <a id=\"decision_tree_clf\"></a>\n",
    "    <img src=\"images/part2_supervised/decision_tree_clf.jpg\" alt=\"Decision Trees\" width=\"60%\" height=\"60%\">\n",
    "    <center><figcaption><em>Figure 9: Decision Tree for Classification</em></figcaption></center>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_decision_tree_classifier(max_depth:int, dataset:pd.DataFrame):\n",
    "    '''\n",
    "    This function splits the dataset into the training and test sets. Then it \n",
    "    creates the plot title. Afterwards, it initializes the decision tree classifier model \n",
    "    and trains it. Then the results are visualized by calling the run_auc_roc_prc() helper\n",
    "    function.\n",
    "\n",
    "    Args:\n",
    "     - max_depth: the depth of the decision tree\n",
    "     - dataset: a pandas dataframe containing the dataset\n",
    "    \n",
    "    Returns: nothing\n",
    "    '''\n",
    "    # split the dataset into training and test set\n",
    "    train_data_features, train_data_labels,\\\n",
    "        test_data_features, test_data_labels \\\n",
    "            = prepare_dataset(label=\"cereals\", original_dataset=dataset)\n",
    "    # create the title of the plot\n",
    "    sup_title = get_plot_titles('Cereals', max_depth=max_depth, \\\n",
    "                                n_estimators=None, clf_type=\"dt\")\n",
    "    # initialize the model\n",
    "    dt_classifier = DecisionTreeClassifier(max_depth=max_depth,\n",
    "                                           random_state=0)\n",
    "    # train the model\n",
    "    dt_classifier.fit(train_data_features, train_data_labels.values.ravel())\n",
    "    # visualize the results \n",
    "    run_auc_roc_prc(dt_classifier, X_train=train_data_features, \n",
    "                                   y_train=train_data_labels, X_test=test_data_features,\n",
    "                                   y_test=test_data_labels, sup_title=sup_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the slider to determine the `max_depth` of the decision tree, and see how this will affect model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_slider_3 = widgets.IntSlider(value=10, min=1, max=20,\\\n",
    "                                        step=1, description='max_depth:')\n",
    "widgets.interact_manual(run_decision_tree_classifier,\n",
    "                        max_depth=max_depth_slider_3,\n",
    "                        dataset=widgets.fixed(dataset)\n",
    "                        );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rf-clf\"></a>\n",
    "### 5.3 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest model for classification is the same as for regression. There will be two hyperparameters to experiment with again: `max_depth` and `n_estimators`. The number of decision trees is determined by the `n_estimators` hyperparameter. Meanwhile, the depth of each decision tree is determined by the `max_depth` hyperparameter. In the below code cells, you can use the sliders to experiment with different values of these hyperparameters and see how they affect the accuracy, F1 score and ROC and PRC areas..\n",
    "<center>\n",
    "    <a id=\"random_forest_clf\"></a>\n",
    "    <img src=\"images/part2_supervised/random_forest_clf.jpg\" alt=\"Random Forest\" width=\"80%\" height=\"60%\">\n",
    "    <center><figcaption><em>Figure 10: Random Forest Model Illustration</em></figcaption></center>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random_forest_classifier(n_estimators:int, max_depth:int, dataset:pd.DataFrame):\n",
    "    '''\n",
    "    This function splits the dataset into the training and test sets. Then it \n",
    "    creates the plot title. Afterwards, it initializes the random forest classifier model \n",
    "    and trains it. Then the results are visualized by calling the run_auc_roc_prc() helper\n",
    "    function.\n",
    "\n",
    "    Args:\n",
    "     - max_depth: the depth of the trees of the random forest\n",
    "     - n_estimators: the number of trees of the random forest\n",
    "     - dataset: a pandas dataframe containing the dataset\n",
    "    \n",
    "    Returns: nothing\n",
    "    '''\n",
    "    # split the dataset into training and test set\n",
    "    train_data_features, train_data_labels,\\\n",
    "        test_data_features, test_data_labels \\\n",
    "            = prepare_dataset(label=\"cereals\", original_dataset=dataset)\n",
    "    # create the title of the plot\n",
    "    sup_title = get_plot_titles('Cereals', n_estimators=n_estimators, \\\n",
    "                                max_depth=max_depth, clf_type=\"rf\")\n",
    "    # initialize the model\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, \n",
    "                                           max_depth=max_depth,\n",
    "                                           random_state=0)\n",
    "    # train the model\n",
    "    rf_classifier.fit(train_data_features, train_data_labels.values.ravel())\n",
    "    # visualize the results\n",
    "    run_auc_roc_prc(rf_classifier, X_train=train_data_features, \n",
    "                                   y_train=train_data_labels, X_test=test_data_features,\n",
    "                                   y_test=test_data_labels, sup_title=sup_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators_slider = widgets.IntSlider(value=30, min=10, max=1000,\\\n",
    "                                         step=5, description='n_estimators:')\n",
    "max_depth_slider_4 = widgets.IntSlider(value=10, min=1, max=20,\\\n",
    "                                        step=1, description='max_depth:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets.interact_manual(run_random_forest_classifier, \n",
    "                        n_estimators=n_estimators_slider,\n",
    "                        max_depth=max_depth_slider_4,\n",
    "                        dataset=widgets.fixed(dataset)\n",
    "                        );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ **CHECKPOINT:**\n",
    "- Compare and contrast the performance of the three types of classifiers. Set `max_depth=4` for both the Decision Tree classifier and the Random Forest classifier. Also, for the random Forest classifier set the `n_estimators=30`. With these hyperparameter settings, compare the performance of the three models. Which performs the best? Where can you see indications of overfitting/underfitting? What are some measures to prevent overfitting/underfitting in our specific case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "## 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module we dove into supervised learning. First, we defined what supervised learning is and its main use cases: classification and regression. Secondly, we explored the training process, the difference between parameters and hyperparameters, and the notion of cost function. Then we moved on to the evaluation part, where we saw different evaluation metrics used in regression and classification. In the last two sections, we experienced different regression and classification models in action.\n",
    "\n",
    "In the next module, we will explore unsupervised learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
