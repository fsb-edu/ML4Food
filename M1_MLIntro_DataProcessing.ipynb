{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Introduction to Machine Learning and Data Processing for Food Sciences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the first module of this series! In this module you will first get an overview on what Machine Learning is and how it can be used in Food Science. Then we will move on to a real-world dataset from the [Swiss Food composition database](https://naehrwertdaten.ch/en/) to learn what data pre-processing steps are needed before applying Machine Learning methods on our data. We will be using this processed dataset in the next two modules on supervised and unsupervised Machine Learning.\n",
    "\n",
    "**Module overview**\n",
    "1. [Introduction to Machine Learning (ML)](#intro)\n",
    "2. [ML in Food Science](#food)\n",
    "3. [Basic data processing for ML](#proc)\n",
    "\n",
    "**Dataset**       \n",
    "Throughout this and future modules we will be working with a dataset from the [Swiss Food composition database](https://naehrwertdaten.ch/en/). The dataset contains information on the composition of different foods that are available in Switzerland and is maintained by the Federal Food Safety and Veterinary Office. In this tutorial you will be given a pre-cleaned version of this dataset. If you are interested in the cleaning performed on the original dataset, feel free to look at the following script: `scripts/Clean_FoodCompositionData.ipynb`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## 1. Introduction to Machine Learning (ML)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning (ML) has become increasingly prevalent in our daily lives. Text writing programs use ML to autocorrect your texts, search engines use ML to improve your search results, and spam in your email inbox is detected by ML. Although ML has gained significant popularity and widespread use in recent years, it is not a new concept. The term was first coined by Arthur Samuel in 1959. He defined Machine Learning as \"the field of study that gives computers the ability to learn without being explicitly programmed\". To rephrase this in simpler terms, one can say that **Machine Learning is the field of study that gives computers the ability to learn from data**. \n",
    "\n",
    "Data is essential for any ML method. Depending on the type of data we have, we differentiate between **supervised and unsupervised ML**. If the data we have is labelled and we know the groups within, we can use supervised ML methods. Conversely, if our data is not labelled or we decide to ignore any known labels, we can employ unsupervised ML techniques to identify patterns within. You will learn more about both two types of ML in future modules."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three distinct steps in **building a ML model** in practice (see [Figure 1](#ml-model)):        \n",
    "**1)** processing the data     \n",
    "**2)** training the ML model    \n",
    "**3)** evaluating the trained model      \n",
    "\n",
    "The quality of the ML model depends a lot on the quality of the data available to train it. Therefore, a lot of time is spent in practice on processing the raw data (**1) PROCESS**). You will learn more about the usual data processing steps in the [respective section later on](#proc). Once the data is processed, it is split into two parts: the train and test set. The train set is used to train the ML model (**2) TRAIN**) whereas the test set is used to evaluate the model performance (**3) EVALUATE**).        \n",
    "Once we have found a ML model that performs well on our test set, we can use it to:           \n",
    "* gain <span style=\"color:darkred\">insights into the processes governing our data</span>\n",
    "* perform <span style=\"color:darkgreen\">predictions on new data</span>.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a id=\"ml-model\"></a>\n",
    "<img src=\"images/part1_intro/ml_pipeline.jpg\" alt=\"Machine Learning Model\" width=\"75%\">\n",
    "<center><figcaption><em>Figure 1: Three steps in building a ML model.</em></figcaption></center>\n",
    "</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸ’¡ CHECKPOINT:**\n",
    " - You have trained a supervised ML model to accurately predict the expiration date of a food product given its chemical composition. What could you do with the trained model if someone gave you an unlabelled set of foods and their compositions?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='food'></a>\n",
    "## 2. ML in Food Science"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">TODO: Input from experts needed.</span> Draft text from Doriela can be found in `notes/m1_ml_in_food_science.txt`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='proc'></a>\n",
    "## 3. Basic data processing for ML"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since ML models are built from data, the performance of the trained models depends on the quality of the data fed into the model. In practice, the data we get from collaborators or the internet might not be standardized. Also, the data might contain erroneous or missing values. Hence, it is crucial to explore and process the data before we use it for training ML models. \n",
    "\n",
    "In this section we will introduce some frequently used data pre-processing methods on the Swiss food composition dataset. The pre-processing methods we will cover include:\n",
    "* [3.2 Data quality control](#quality)\n",
    "* [3.3 Train-test split](#train-test-split)\n",
    "* [3.4 Standardization](#stand)\n",
    "\n",
    "Before diving into these processing methods, we will explore what our dataset is composed of in section [3.1 Anatomy of our data](#anatomy).\n",
    "\n",
    "To explore and process our dataset we will make use of different Python modules that are imported in the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to read the dataset\n",
    "import os\n",
    "\n",
    "# Pandas is used to read and interact with our data across all processing steps\n",
    "# we import it with an alias \"pd\"\n",
    "import pandas as pd\n",
    "\n",
    "# The below packages are additionally needed for sections 3.2 - 3.4\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Packages needed for visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the above cell finishes running successfully, you will see `[1]`, in the bottom left corner of the box. This means that the packages are loaded and ready to be used."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Anatomy of our data <a id='anatomy'></a> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By data we refer to the collection of samples and features. Each **sample** represents a row in our table and is denoted with a unique ID. For each sample, we get a set of **features** that were measured. Features represent certain characteristics of our samples. \n",
    "\n",
    "Let us illustrate samples and features structure on our Swiss food composition dataset. To do this, we first have to **load the data**. The dataset is stored in a `.csv` (comma separated values) file and we will save it in a **pandas dataframe**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(\"data\", \"swiss_food_composition.csv\")\n",
    "dataset = pd.read_csv(filepath, index_col=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find how many samples and features we have in our dataset, we can use the `shape` attribute of the `dataset` dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1092, 41)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we know that there are 1092 samples (rows) and 41 features (columns) in our dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how the first 5 rows of our dataset look we can use the `head()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>energy_kcal</th>\n",
       "      <th>fat_g</th>\n",
       "      <th>fatty_acids_sat_g</th>\n",
       "      <th>fatty_acids_monounsat_g</th>\n",
       "      <th>fatty_acids_polyunsat_g</th>\n",
       "      <th>cholesterol_mg</th>\n",
       "      <th>carbohydrates_g</th>\n",
       "      <th>sugars_g</th>\n",
       "      <th>...</th>\n",
       "      <th>potassium_mg</th>\n",
       "      <th>sodium_mg</th>\n",
       "      <th>chloride_mg</th>\n",
       "      <th>calcium_mg</th>\n",
       "      <th>magnesium_mg</th>\n",
       "      <th>phosphorus_mg</th>\n",
       "      <th>iron_mg</th>\n",
       "      <th>iodide_Âµg</th>\n",
       "      <th>zinc_mg</th>\n",
       "      <th>selenium_Âµg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agar Agar</td>\n",
       "      <td>other</td>\n",
       "      <td>160.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>52.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>660.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Agave syrup</td>\n",
       "      <td>sweets</td>\n",
       "      <td>293.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Almond</td>\n",
       "      <td>fruits</td>\n",
       "      <td>624.0</td>\n",
       "      <td>52.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>31.4</td>\n",
       "      <td>11.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>6.6</td>\n",
       "      <td>...</td>\n",
       "      <td>740.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>40.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>510.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Almond, dry roasted, salted</td>\n",
       "      <td>nuts</td>\n",
       "      <td>637.0</td>\n",
       "      <td>52.5</td>\n",
       "      <td>4.1</td>\n",
       "      <td>33.1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>...</td>\n",
       "      <td>710.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>1190.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Almond, roasted, salted</td>\n",
       "      <td>nuts</td>\n",
       "      <td>649.0</td>\n",
       "      <td>55.2</td>\n",
       "      <td>4.2</td>\n",
       "      <td>34.8</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>4.6</td>\n",
       "      <td>...</td>\n",
       "      <td>670.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>1190.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           name category  energy_kcal  fat_g  \\\n",
       "ID                                                             \n",
       "0                     Agar Agar    other        160.0    0.2   \n",
       "1                   Agave syrup   sweets        293.0    0.0   \n",
       "2                        Almond   fruits        624.0   52.1   \n",
       "3   Almond, dry roasted, salted     nuts        637.0   52.5   \n",
       "4       Almond, roasted, salted     nuts        649.0   55.2   \n",
       "\n",
       "    fatty_acids_sat_g  fatty_acids_monounsat_g  fatty_acids_polyunsat_g  \\\n",
       "ID                                                                        \n",
       "0                 NaN                      NaN                      NaN   \n",
       "1                 0.0                      NaN                      NaN   \n",
       "2                 4.1                     31.4                     11.4   \n",
       "3                 4.1                     33.1                     13.0   \n",
       "4                 4.2                     34.8                     13.5   \n",
       "\n",
       "    cholesterol_mg  carbohydrates_g  sugars_g  ...  potassium_mg  sodium_mg  \\\n",
       "ID                                             ...                            \n",
       "0              NaN              0.0       NaN  ...          52.0      130.0   \n",
       "1              NaN             73.1       NaN  ...           NaN        4.0   \n",
       "2              0.0              7.8       6.6  ...         740.0        1.1   \n",
       "3              0.0             10.1       4.9  ...         710.0      230.0   \n",
       "4              0.0              7.2       4.6  ...         670.0      330.0   \n",
       "\n",
       "    chloride_mg  calcium_mg  magnesium_mg  phosphorus_mg  iron_mg  iodide_Âµg  \\\n",
       "ID                                                                             \n",
       "0           NaN       660.0         100.0           34.0      4.5        NaN   \n",
       "1           NaN         NaN           NaN            NaN      NaN        NaN   \n",
       "2          40.0       270.0         240.0          510.0      3.3        0.2   \n",
       "3        1190.0       270.0         280.0          470.0      3.7        2.4   \n",
       "4        1190.0       240.0         270.0          470.0      3.3        2.4   \n",
       "\n",
       "    zinc_mg  selenium_Âµg  \n",
       "ID                        \n",
       "0       1.5          NaN  \n",
       "1       NaN          NaN  \n",
       "2       3.3          2.2  \n",
       "3       3.3          2.0  \n",
       "4       3.1          2.0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how the last 5 rows of our dataset look we can use the `tail()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>energy_kcal</th>\n",
       "      <th>fat_g</th>\n",
       "      <th>fatty_acids_sat_g</th>\n",
       "      <th>fatty_acids_monounsat_g</th>\n",
       "      <th>fatty_acids_polyunsat_g</th>\n",
       "      <th>cholesterol_mg</th>\n",
       "      <th>carbohydrates_g</th>\n",
       "      <th>sugars_g</th>\n",
       "      <th>...</th>\n",
       "      <th>potassium_mg</th>\n",
       "      <th>sodium_mg</th>\n",
       "      <th>chloride_mg</th>\n",
       "      <th>calcium_mg</th>\n",
       "      <th>magnesium_mg</th>\n",
       "      <th>phosphorus_mg</th>\n",
       "      <th>iron_mg</th>\n",
       "      <th>iodide_Âµg</th>\n",
       "      <th>zinc_mg</th>\n",
       "      <th>selenium_Âµg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>Zucchini piccata, prepared</td>\n",
       "      <td>other</td>\n",
       "      <td>124.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>...</td>\n",
       "      <td>210.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>Zucchini slices, breaded, prepared</td>\n",
       "      <td>other</td>\n",
       "      <td>127.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>44.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>2.1</td>\n",
       "      <td>...</td>\n",
       "      <td>210.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>Zucchini, raw</td>\n",
       "      <td>vegetables</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>230.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>Zucchini, steamed (without addition of salt)</td>\n",
       "      <td>vegetables</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>...</td>\n",
       "      <td>220.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>26.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>Zucchini, stewed (without addition of fat and ...</td>\n",
       "      <td>vegetables</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>...</td>\n",
       "      <td>240.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>30.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   name    category  \\\n",
       "ID                                                                    \n",
       "1087                         Zucchini piccata, prepared       other   \n",
       "1088                 Zucchini slices, breaded, prepared       other   \n",
       "1089                                      Zucchini, raw  vegetables   \n",
       "1090       Zucchini, steamed (without addition of salt)  vegetables   \n",
       "1091  Zucchini, stewed (without addition of fat and ...  vegetables   \n",
       "\n",
       "      energy_kcal  fat_g  fatty_acids_sat_g  fatty_acids_monounsat_g  \\\n",
       "ID                                                                     \n",
       "1087        124.0    8.0                2.2                      4.0   \n",
       "1088        127.0    5.5                0.7                      3.4   \n",
       "1089         19.0    0.2                0.0                      0.0   \n",
       "1090         20.0    0.2                0.0                      0.0   \n",
       "1091         23.0    0.2                0.0                      0.0   \n",
       "\n",
       "      fatty_acids_polyunsat_g  cholesterol_mg  carbohydrates_g  sugars_g  ...  \\\n",
       "ID                                                                        ...   \n",
       "1087                      1.0            65.0              6.6       1.8  ...   \n",
       "1088                      0.9            44.0             13.5       2.1  ...   \n",
       "1089                      0.1             0.0              2.0       1.9  ...   \n",
       "1090                      0.1             0.0              2.2       2.1  ...   \n",
       "1091                      0.1             0.0              2.4       2.3  ...   \n",
       "\n",
       "      potassium_mg  sodium_mg  chloride_mg  calcium_mg  magnesium_mg  \\\n",
       "ID                                                                     \n",
       "1087         210.0       93.0        150.0        76.0          24.0   \n",
       "1088         210.0       89.0        160.0        28.0          24.0   \n",
       "1089         230.0        3.0         24.0        19.0          23.0   \n",
       "1090         220.0        2.9         26.0        21.0          24.0   \n",
       "1091         240.0        3.4         30.0        24.0          25.0   \n",
       "\n",
       "      phosphorus_mg  iron_mg  iodide_Âµg  zinc_mg  selenium_Âµg  \n",
       "ID                                                             \n",
       "1087           97.0      0.9       11.0      0.7          NaN  \n",
       "1088           65.0      1.0        7.3      0.5          NaN  \n",
       "1089           31.0      0.8        2.3      0.2          NaN  \n",
       "1090           33.0      0.7        2.5      0.3          NaN  \n",
       "1091           39.0      0.9        2.9      0.3          NaN  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the above data excerpts that each sample is linked with a unique row `ID` ranging from `1` to `1091`. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data Quality Control <a id='quality'></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in the data pre-processing phase is to assess the quality of our data. There are different ways to inspect this and here we will explore some of them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find number of missing values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by getting some information about what kind of data is contained in the dataset. For this we will use the `info()` method of the `dataset`. Besides listing the data types of each column, it shows how many non-empy (`non-null`) values are there for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1092 entries, 0 to 1091\n",
      "Data columns (total 41 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   name                       1092 non-null   object \n",
      " 1   category                   1092 non-null   object \n",
      " 2   energy_kcal                1092 non-null   float64\n",
      " 3   fat_g                      1092 non-null   float64\n",
      " 4   fatty_acids_sat_g          1087 non-null   float64\n",
      " 5   fatty_acids_monounsat_g    1085 non-null   float64\n",
      " 6   fatty_acids_polyunsat_g    1085 non-null   float64\n",
      " 7   cholesterol_mg             1080 non-null   float64\n",
      " 8   carbohydrates_g            1092 non-null   float64\n",
      " 9   sugars_g                   1087 non-null   float64\n",
      " 10  starch_g                   1038 non-null   float64\n",
      " 11  fibres_g                   1091 non-null   float64\n",
      " 12  protein_g                  1092 non-null   float64\n",
      " 13  salt_g                     1092 non-null   float64\n",
      " 14  alcohol_g                  1088 non-null   float64\n",
      " 15  water_g                    1090 non-null   float64\n",
      " 16  vit_A_activity_re_Âµg       1079 non-null   float64\n",
      " 17  vit_A_activity_rae_Âµg      1079 non-null   float64\n",
      " 18  retinol_Âµg                 1082 non-null   float64\n",
      " 19  beta_carotene_activity_Âµg  1073 non-null   float64\n",
      " 20  beta_carotene_Âµg           1071 non-null   float64\n",
      " 21  vit_B1_mg                  1085 non-null   float64\n",
      " 22  vit_B2_mg                  1086 non-null   float64\n",
      " 23  vit_B6_mg                  1085 non-null   float64\n",
      " 24  vit_B12_Âµg                 1082 non-null   float64\n",
      " 25  niacin_mg                  1052 non-null   float64\n",
      " 26  folate_Âµg                  1075 non-null   float64\n",
      " 27  panthotenic_acid_mg        1080 non-null   float64\n",
      " 28  vit_c_mg                   1079 non-null   float64\n",
      " 29  vit_d_Âµg                   1081 non-null   float64\n",
      " 30  vit_e_activity_mg          1085 non-null   float64\n",
      " 31  potassium_mg               1087 non-null   float64\n",
      " 32  sodium_mg                  1092 non-null   float64\n",
      " 33  chloride_mg                1068 non-null   float64\n",
      " 34  calcium_mg                 1087 non-null   float64\n",
      " 35  magnesium_mg               1085 non-null   float64\n",
      " 36  phosphorus_mg              1086 non-null   float64\n",
      " 37  iron_mg                    1087 non-null   float64\n",
      " 38  iodide_Âµg                  1071 non-null   float64\n",
      " 39  zinc_mg                    1086 non-null   float64\n",
      " 40  selenium_Âµg                360 non-null    float64\n",
      "dtypes: float64(39), object(2)\n",
      "memory usage: 358.3+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the columns hold decimal values, which in Python correspond to the **float64** data types.\n",
    "The first two columns, `Name` and `Category` are `strings` (sequences of characters) and `pandas` recognizes them as objects."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing values in the dataset are recorded as `NaN`s. Here is an example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>energy_kcal</th>\n",
       "      <th>fat_g</th>\n",
       "      <th>fatty_acids_sat_g</th>\n",
       "      <th>fatty_acids_monounsat_g</th>\n",
       "      <th>fatty_acids_polyunsat_g</th>\n",
       "      <th>cholesterol_mg</th>\n",
       "      <th>carbohydrates_g</th>\n",
       "      <th>sugars_g</th>\n",
       "      <th>...</th>\n",
       "      <th>potassium_mg</th>\n",
       "      <th>sodium_mg</th>\n",
       "      <th>chloride_mg</th>\n",
       "      <th>calcium_mg</th>\n",
       "      <th>magnesium_mg</th>\n",
       "      <th>phosphorus_mg</th>\n",
       "      <th>iron_mg</th>\n",
       "      <th>iodide_Âµg</th>\n",
       "      <th>zinc_mg</th>\n",
       "      <th>selenium_Âµg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agar Agar</td>\n",
       "      <td>other</td>\n",
       "      <td>160.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>52.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>660.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         name category  energy_kcal  fat_g  fatty_acids_sat_g  \\\n",
       "ID                                                              \n",
       "0   Agar Agar    other        160.0    0.2                NaN   \n",
       "\n",
       "    fatty_acids_monounsat_g  fatty_acids_polyunsat_g  cholesterol_mg  \\\n",
       "ID                                                                     \n",
       "0                       NaN                      NaN             NaN   \n",
       "\n",
       "    carbohydrates_g  sugars_g  ...  potassium_mg  sodium_mg  chloride_mg  \\\n",
       "ID                             ...                                         \n",
       "0               0.0       NaN  ...          52.0      130.0          NaN   \n",
       "\n",
       "    calcium_mg  magnesium_mg  phosphorus_mg  iron_mg  iodide_Âµg  zinc_mg  \\\n",
       "ID                                                                         \n",
       "0        660.0         100.0           34.0      4.5        NaN      1.5   \n",
       "\n",
       "    selenium_Âµg  \n",
       "ID               \n",
       "0           NaN  \n",
       "\n",
       "[1 rows x 41 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.iloc[0:1, :]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `Agar Agar` there is no value for `fatty_acids_sat_g`, `fatty_acids_monounsat_g`, `fatty_acids_polyunsat_g`, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing features with a lot of missing values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features (columns) that have a lot of missing values will not be useful for the models, so we can remove them. Determining what *a lot* means depends on the context and task at hand. In our case, we will remove the features that have more than 20% of the values missing. For this we will first find the actual minimum number of rows that have to contain a value per feature. This will be the `feature_threshold`. \n",
    "\n",
    "Since we will remove features that have more than 20% of the data missing, we will keep those that have at least 80%. We have 1092 samples in total, this means that we will keep features that have at least $$\\left\\lceil 0.8*1092=873.6\\right\\rceil = 874$$ non-missing values. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will print the number of `NaN` values per column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name                           0\n",
       "category                       0\n",
       "energy_kcal                    0\n",
       "fat_g                          0\n",
       "fatty_acids_sat_g              5\n",
       "fatty_acids_monounsat_g        7\n",
       "fatty_acids_polyunsat_g        7\n",
       "cholesterol_mg                12\n",
       "carbohydrates_g                0\n",
       "sugars_g                       5\n",
       "starch_g                      54\n",
       "fibres_g                       1\n",
       "protein_g                      0\n",
       "salt_g                         0\n",
       "alcohol_g                      4\n",
       "water_g                        2\n",
       "vit_A_activity_re_Âµg          13\n",
       "vit_A_activity_rae_Âµg         13\n",
       "retinol_Âµg                    10\n",
       "beta_carotene_activity_Âµg     19\n",
       "beta_carotene_Âµg              21\n",
       "vit_B1_mg                      7\n",
       "vit_B2_mg                      6\n",
       "vit_B6_mg                      7\n",
       "vit_B12_Âµg                    10\n",
       "niacin_mg                     40\n",
       "folate_Âµg                     17\n",
       "panthotenic_acid_mg           12\n",
       "vit_c_mg                      13\n",
       "vit_d_Âµg                      11\n",
       "vit_e_activity_mg              7\n",
       "potassium_mg                   5\n",
       "sodium_mg                      0\n",
       "chloride_mg                   24\n",
       "calcium_mg                     5\n",
       "magnesium_mg                   7\n",
       "phosphorus_mg                  6\n",
       "iron_mg                        5\n",
       "iodide_Âµg                     21\n",
       "zinc_mg                        6\n",
       "selenium_Âµg                  732\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_total_col = dataset.isna().sum(axis=0)\n",
    "nan_total_col"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the list above, we will remove all columns that have more than 1092-874 = 218 missing values. Only the `selenium_Âµg` has more than 218 missing values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code cell does the following:\n",
    "\n",
    "It checks each value of the dataset and marks it with 1 if it is a `nan` and with a 0 otherwise. Then, it computes the sum of each column using the `sum()` method with the `axis=0`. In this way we find the number of 1s per column, which means the number of values that are `nan`s per column. You can find a visual explanation on [Figure 2](#boolean-mask-illustration) below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can drop the columns that have more than 20% of the values missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "874"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage_present = 0.8\n",
    "feature_threshold = math.ceil(percentage_present*dataset.shape[0]) # 20% of values missing\n",
    "feature_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['name', 'category', 'energy_kcal', 'fat_g', 'fatty_acids_sat_g',\n",
       "       'fatty_acids_monounsat_g', 'fatty_acids_polyunsat_g', 'cholesterol_mg',\n",
       "       'carbohydrates_g', 'sugars_g', 'starch_g', 'fibres_g', 'protein_g',\n",
       "       'salt_g', 'alcohol_g', 'water_g', 'vit_A_activity_re_Âµg',\n",
       "       'vit_A_activity_rae_Âµg', 'retinol_Âµg', 'beta_carotene_activity_Âµg',\n",
       "       'beta_carotene_Âµg', 'vit_B1_mg', 'vit_B2_mg', 'vit_B6_mg', 'vit_B12_Âµg',\n",
       "       'niacin_mg', 'folate_Âµg', 'panthotenic_acid_mg', 'vit_c_mg', 'vit_d_Âµg',\n",
       "       'vit_e_activity_mg', 'potassium_mg', 'sodium_mg', 'chloride_mg',\n",
       "       'calcium_mg', 'magnesium_mg', 'phosphorus_mg', 'iron_mg', 'iodide_Âµg',\n",
       "       'zinc_mg'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_before = dataset.columns\n",
    "dataset = dataset.dropna(axis=1, thresh=feature_threshold)\n",
    "columns_after = dataset.columns\n",
    "columns_after"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first code cell we declare a variable, `feature_threshold = math.ceil(0.8*dataset.shape[0])` to be the threshold according to which we are discarding columns. It gets the number of samples using the first entry of the `dataset.shape` attribute which returns a `tuple`. Then, we save the columns we had to see later what was removed. Afterwards, we apply the `dropna()` method to drop all columns (determined by the `axis=1` argument), that have less than `feature_threshold` non-missing values. We save the columns after the drop as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['selenium_Âµg'], dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_before.difference(columns_after)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `difference` method does set difference on the two lists specified. As you can see, `selenium_Âµg` is not present anymore in the columns list. The code cell below counts the number of columns. Now we have 40 columns, from 41 that we had before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing samples with a lot of missing values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove the samples that have more than  20% of the values missing. This means that we will keep the samples that have more than 80% of the features. With some quick math we find that: $$\\left\\lceil0.8*41 = 32.8 \\right\\rceil = 33$$ This means that we will keep the samples that have 33 or more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "0       24\n",
       "1       30\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "1087     0\n",
       "1088     0\n",
       "1089     0\n",
       "1090     0\n",
       "1091     0\n",
       "Length: 1092, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_total_rows = dataset.isna().sum(axis=1)\n",
    "nan_total_rows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell above finds the number of missing values per each sample. This time we sum row-wise by specifying the `axis=1` argument in the `sum()` method. [Figure 2](#boolean-mask-illustration) illustrates how the `isna()` and `sum()` methods work to calculate the number of `NaN`s per each sample and per each feature. Note that we just show an example of the output for the first 3 samples and 8 columns. After creating the boolean masks ('tables' with 0s and 1s), it finds the sum per row or per column, based on the `axis` value we pass as argument to `sum()` and find the number of missing values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <a id=\"boolean-mask-illustration\"></a>\n",
    "    <img src=\"images/part1_intro/rows_columns_nans_v3.jpg\" alt=\"Rows and columns boolean mask\" width=\"100%\">\n",
    "    <center><figcaption><em>Figure 2: Compute number of missing values per row and column</em></figcaption></center>\n",
    "</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will remove the rows that have more than 20% of the values missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_threshold = math.ceil(percentage_present*dataset.shape[1]) # 20% of values missing\n",
    "row_threshold"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We declare `row_threshold = math.ceil(0.8*dataset.shape[1])` to be the threshold according to which we are discarding rows. It gets the number of features using the second entry of the `dataset.shape` attribute which returns a `tuple`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1084, 40)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_before = dataset.index\n",
    "dataset = dataset.dropna(axis=0, thresh=row_threshold)\n",
    "rows_after = dataset.index\n",
    "dataset.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 1092 samples, now we have 1084."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([0, 1, 83, 84, 898, 899, 1022, 1057], dtype='int64', name='ID')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_before.difference(rows_after)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we apply the `difference()` method to find the positions of the samples that were dropped."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the features and samples that had a lot of missing values, the dataset has 1084 samples and 40 features. Note that our dataset still has missing values and we will deal with them in the subsequent sections."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlier detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers are samples that differ from the majority of the samples in the dataset. Most of the machine learning models are sensitive to outliers since they affect the learning process. Thus it is important to deal with them. **The process of identifying and handling outliers is called \"outlier detection\".** \n",
    "\n",
    "There are multiple ways to detect outliers. They include:\n",
    " - **statistical approaches**: calculating mean, standard deviation, quartiles or percentiles of features and identifying data points that fall outside certain ranges or thresholds,\n",
    " - **distance-based methods**: e.g using distances like Euclidean distance to find samples that are far away,\n",
    " - **machine learning techniques**: using clustering, anomaly detection algorithms, etc.\n",
    " \n",
    " You can find a more detailed description of outlier detection methods [here](https://www.mygreatlearning.com/blog/what-is-outlier-detection/) and [here](https://towardsdatascience.com/how-to-identify-outliers-in-data-with-python-a9fe40235064).\n",
    "\n",
    "\n",
    "There are multiple ways to deal with the outliers, too. The easiest of them is to completely remove them. However, we should be careful with this approach since it may result in loss of useful information. Also, if the dataset is small, removing the outliers will make it even smaller. Another approach is to use statistical models or machine learning algorithms that are robust to outliers. More  information on outlier handling techniques can be found [here](https://towardsdatascience.com/dont-throw-away-your-outliers-c37e1ab0ce19).\n",
    "\n",
    "\n",
    "It depends on the dataset which combination of the above techniques is best suited. Often, researchers test multiple techniques before making a decision.\n",
    "\n",
    "Our food composition dataset does not contain any outliers. Hence, we will not be performing any outlier detection here. If you are interested in looking into outlier detection methods more closely, you can find them demonstrated with coding examples [here](https://towardsdatascience.com/5-outlier-detection-methods-that-every-data-enthusiast-must-know-f917bf439210)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with missing data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to deal with missing data. The easiest and most drastic one is to completely remove the samples that have missing data. We cannot apply this here because we would have to delete more than 70% of the dataset, meaning that we would have too few samples to do any meaningful analysis. Other approaches are possible for imputing the dataset, e.g. replace the missing values with some characteristic values of the features. For example, we can decide to substitute missing values in a column with the mean value for that feature. Likewise we can substitute them with the median or mode values. You can find more imputation techniques in the [sklearn documentation](https://scikit-learn.org/stable/modules/impute.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will impute the missing values with the mean of the features, but for each food category. Let us check which food categories do we have in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fruits', 'nuts', 'cereals', 'sweets', 'other', 'dairy',\n",
       "       'non_alcoholic_beverages', 'vegetables', 'meat', 'herbs', 'sauce',\n",
       "       'alcoholic_beverages'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['category'].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code cell does the following: it first selects all the rows of the `category` column of the dataset and then find all unique values in this column. There are 12 unique food categories in our dataset. In this case, we will find the mean value per food category and per feature, and aftewards, subsitute the missing values with the per category and per feature mean.\n",
    "Let us look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "starch_g     NaN\n",
       "category    meat\n",
       "Name: 221, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.loc[221, ['starch_g', 'category']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample, that corresponds to the `meat` category has a missing value for the feature `starch_g`.  Using the `loc` indexer we can select the row 221, and for that row we get only the values that correspond to the  `'starch_g'` ,`'category'` columns. This value will be imputed with the mean of all the `starch_g` values for all food samples belonging to the `meat` category.\n",
    "To do so we will write a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(dataset:pd.DataFrame, \n",
    "                          category_col:str, \n",
    "                          feature_col_names:list)->pd.DataFrame:\n",
    "    modified_dataset = dataset.copy()\n",
    "    for feature in feature_col_names:\n",
    "            modified_dataset[feature] = modified_dataset.groupby(category_col)[feature]\\\n",
    "                                                        .transform(lambda x: x.fillna(x.mean()))\n",
    "    return modified_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes as arguments the dataset that we are imputing, the name of the column that specifies the category according to which we are grouping the food samples and the feature columns (all the numeric columns) were missing values may be present.\n",
    "\n",
    "First a copy of the dataset is created, to not modify the original one. Then, we iterate over all the numerical features of the dataset. For each feature, we group the dataset by the `category` column. This means that all samples belonging to the same category will be consecutive. This is achieved by `modified_dataset.groupby(category_col)[feature]`. Then for each of these groups we find the mean of all non-missing values and at the same time we fill the missing values of that group with the mean. This is achieved by `transform(lambda x: x.fillna(x.mean()))`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we use the function, we will define the names of the columns that contain numerical features and save them in the `cols` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['energy_kcal', 'fat_g', 'fatty_acids_sat_g', 'fatty_acids_monounsat_g',\n",
       "       'fatty_acids_polyunsat_g', 'cholesterol_mg', 'carbohydrates_g',\n",
       "       'sugars_g', 'starch_g', 'fibres_g', 'protein_g', 'salt_g', 'alcohol_g',\n",
       "       'water_g', 'vit_A_activity_re_Âµg', 'vit_A_activity_rae_Âµg',\n",
       "       'retinol_Âµg', 'beta_carotene_activity_Âµg', 'beta_carotene_Âµg',\n",
       "       'vit_B1_mg', 'vit_B2_mg', 'vit_B6_mg', 'vit_B12_Âµg', 'niacin_mg',\n",
       "       'folate_Âµg', 'panthotenic_acid_mg', 'vit_c_mg', 'vit_d_Âµg',\n",
       "       'vit_e_activity_mg', 'potassium_mg', 'sodium_mg', 'chloride_mg',\n",
       "       'calcium_mg', 'magnesium_mg', 'phosphorus_mg', 'iron_mg', 'iodide_Âµg',\n",
       "       'zinc_mg'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = dataset.select_dtypes(include='number').columns\n",
    "cols"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass the argument `'number'` to the `select_dtypes` method of Pandas dataframes to select the part of the dataset that contains only numeric values. After that, we select only the corresponding column names using the `columns` attribute."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now imputation is performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = impute_missing_values(dataset, category_col='category', feature_col_names=cols)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm that the function has imputed the `NaN` values, let's look at the example from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "starch_g    0.265962\n",
       "category        meat\n",
       "Name: 221, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.loc[221, ['starch_g', 'category']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it has the mean starch value for all meat products. To verify that there are no `NaN` values left, we can run this code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 40)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a boolean mask for NaN values\n",
    "nan_mask = dataset.isna().any(axis=1)\n",
    "\n",
    "# create a new DataFrame with samples containing NaN values\n",
    "df_with_nan_only = dataset[nan_mask]\n",
    "\n",
    "# check the dimensions\n",
    "df_with_nan_only.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the resulting dataframe has zero rows. This means that there are no missing values left."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ **CHECKPOINT:**\n",
    "- Why is data pre-processing important?\n",
    "- Why do we need to remove samples/features with a lot of missing values?\n",
    "- Why is imputation done per food category?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Train-Test Split <a id='train-test-split'></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although in practice it happens after the real data preprocessing, sometimes it is considered to be part of the data pre-processing phase. As you can see from [Fig. 1](#ml-model), before the machine learning model is trained, the data is split in two parts: the **training data** and the **testing data**. Usually we use **70-80%** of the whole dataset for training and the remaining **20-30%** for testing. This is done to evaluate how well the model performs on an unseen dataset (the test set). If the model performs well on the test set, it has learned the data well and also manages to generalize to unseen data. If the model performance on the test set is poor, the model does not manage to generalize to unseen data and might have learned the training data too well (almost learned by heart)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make an analogy to your courses at ETH. When you go and sit in an exam, you rarely or never find exactly the same questions that you have seen during the exercise sessions or classes. Also, you never know in advance which questions will be asked in the exam. The reason for all this constraints is that the professors want to evaluate you based on what you have understood from the course. If you already knew the questions of the exam you can easily prepare them beforehand, try to memorize the answers even if you understand nothing and then you get a really good grade from the course. But what happens next is that when you start working outside the doors of the university, you will probably not be able to solve the tasks at hand because of lack of understanding of certain concepts and set of skills. \n",
    "\n",
    "This is exactly what we want to avoid when training machine learning models. In the machine learning jargon, we want to have models that generalize well. This means that they can properly solve tasks which they have not encountered during training. And they can do perfectly so, if the training was successful and they have gained insights on data. In our analogy from above, if the students understand the concepts and can put them to practice, then they can be flexible in their jobs and solve tasks that they might not have encountered during the studies. All this happens because they will have the necessary set of skills from their training (education) years. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see how we can split a dataset in practice. We will try two different methods for the train-test split. The first one is the simple train-test split using the `train_test_split()` function and the second one is the stratified train-test split using the again the same function but with an additional parameter, `stratify`. The difference between the two cases is that in the first case, the split of samples between train and test is random, while in the second case the proportions of categories in the train and test sets are preserved. This means that if 10% of the samples in the train set belong to the `meat` category, then approximately 10% of the test samples will belong to the `meat` category, as well.\n",
    "\n",
    "We will start with the simple `train_test_split()` method from the `model_selection` module of the `sklearn` library, that we have imported above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_simple, test_set_simple = train_test_split(dataset, test_size=0.2, random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass three arguments to the method. The first one is the dataset that will be split. The second one `test_size` determines the portion of the data that will be used for testing. Here we specify that `20%=0.2` will be used for testing, thus implicitly the remaining 80% for training. The method, depending on whether we pass the whole dataset or the features and the labels separately, will return 2 new dataframes in the first case (1x for the train set and 1x for the test set) or 4 new dataframes in the second case (1x for the train_features, 1x for the test features, 1x for the train labels and 1x for the test labels). The third argument, `random_state=0` makes sure that no matter how many times we execute the above code cell, it will produce the same split all the times. This means that the `train_set` and `test_set` will always have the same samples. If we omit that argument, then `train_set` and `test_set` will have different samples each time we execute the cell.\n",
    "\n",
    "In order to make sure that the split is correct let us check the sizes of the new dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of original dataset:  (1084, 40)\n",
      "Shape of train dataset:  (867, 40)\n",
      "Shape of test dataset:  (217, 40)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of original dataset: \", dataset.shape)\n",
    "print(\"Shape of train dataset: \", train_set_simple.shape)\n",
    "print(\"Shape of test dataset: \", test_set_simple.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms our desired proportions for the split (80% train and 20% test). However, let us check the percentage of each class in the train and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "other                      19.838524\n",
       "meat                       17.647059\n",
       "cereals                    13.264129\n",
       "vegetables                 11.995386\n",
       "sweets                     11.188005\n",
       "fruits                      9.573241\n",
       "dairy                       9.457901\n",
       "non_alcoholic_beverages     2.883506\n",
       "alcoholic_beverages         2.076125\n",
       "sauce                       1.268743\n",
       "herbs                       0.692042\n",
       "nuts                        0.115340\n",
       "Name: category, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_set_simple['category'].value_counts() / train_set_simple.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "other                      25.806452\n",
       "meat                       15.668203\n",
       "cereals                    14.285714\n",
       "vegetables                 10.599078\n",
       "dairy                       9.677419\n",
       "sweets                      9.677419\n",
       "fruits                      7.373272\n",
       "sauce                       2.304147\n",
       "alcoholic_beverages         1.843318\n",
       "non_alcoholic_beverages     1.382488\n",
       "nuts                        0.921659\n",
       "herbs                       0.460829\n",
       "Name: category, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_set_simple['category'].value_counts() / test_set_simple.shape[0])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases we used the `value_counts()` method to obtain the counts for each category. Then we divided these counts with the number of samples in the respective train and test sets by using the first entry of the `shape` attribute, which corresponds to the number of rows the set will have. Lastly, to convert the numbers to percentages we multiply them by 100. Thus we get the above values. As we can see the category percentages are not preserved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will do the same procedure, but this time adding the `stratify` parameter to the `train_test_split` method. Here we set it to the `category` column of the dataset, since that will determine the proportions that should be preserved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_stratified, test_set_stratified = train_test_split(dataset, test_size=0.2, stratify=dataset['category'], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "other                      20.991926\n",
       "meat                       17.301038\n",
       "cereals                    13.494810\n",
       "vegetables                 11.764706\n",
       "sweets                     10.841984\n",
       "dairy                       9.457901\n",
       "fruits                      9.111880\n",
       "non_alcoholic_beverages     2.537486\n",
       "alcoholic_beverages         2.076125\n",
       "sauce                       1.499423\n",
       "herbs                       0.692042\n",
       "nuts                        0.230681\n",
       "Name: category, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_set_stratified['category'].value_counts() / train_set_stratified.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "other                      21.198157\n",
       "meat                       17.050691\n",
       "cereals                    13.364055\n",
       "vegetables                 11.520737\n",
       "sweets                     11.059908\n",
       "dairy                       9.677419\n",
       "fruits                      9.216590\n",
       "non_alcoholic_beverages     2.764977\n",
       "alcoholic_beverages         1.843318\n",
       "sauce                       1.382488\n",
       "herbs                       0.460829\n",
       "nuts                        0.460829\n",
       "Name: category, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_set_stratified['category'].value_counts() / test_set_stratified.shape[0])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the category percentages are more or less the same. From now on, we will use the stratified train-test split, since it will be useful in the next modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set_stratified.copy()\n",
    "test_set = test_set_stratified.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ **CHECKPOINT:**\n",
    "- Why do we do the train-test split?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Standardization <a id='stand'></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization is the process of transforming or rescaling the values of certain features so that they are comparable. This process is useful to the underlying steps of machine learning algorithms in finding the best model that can generalize the data. The formula for standardizing the values of a single feature column would be: $$ X^{(i)}_s = \\frac{X^{(i)}-\\mu}{\\sigma} ,$$ where $X^{(i)}_s$ is the standardized value for sample `i`, $X^{(i)}$ is the original value of the feature for sample `i`, $\\mu$ is the mean of all values of the feature (the whole column) and $\\sigma$ is the standard deviation of all the values of that feature. We can depict this graphically as follows:\n",
    "\n",
    "<center>\n",
    "    <a id=\"standardization-illustration\"></a>\n",
    "    <img src=\"images/part1_intro/standardization.jpg\" alt=\"Standardization\" width=\"75%\">\n",
    "    <center><figcaption><em>Figure 3: Standardization in practice</em></figcaption></center>\n",
    "</center>\n",
    "\n",
    "As you can see, we first find the mean, column-wise for a feature. Then we find the standard deviation and then we apply the above formula for each of the samples to transform that feature into a standardized one. We repeat this procedure for all the feature columns one by one. In addition, standardization makes the distributions of the features more uniform.\n",
    "\n",
    "**NOTE**: If we are in a supervised learning task, then we apply the scaling to all features but not to the label/target column. We avoid this because it might change the interpretation of the target variable. While in the case of unsupervised learning, the standardization may be applied to all columns since there is no target column."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this method, the mean of each corresponding feature is subtracted from the values and each of them is scaled to unit variance. Moreover, by using standardization the learning models will not be affected a lot by outliers and the learning process will be smoother. This happens due to some underlying properties of the standard normal distribution that are out of the scope of this notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either code the procedure by ourselves or we can use ready-made code from the `sklearn` module. Here you will see an example of how to do standardization using `StandardScaler` from `sklearn`. We will continue using the dataset from the previous section. \n",
    "Again, we will start by importing the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = StandardScaler()\n",
    "train_set_numerical_st = standard_scaler.fit_transform(train_set[cols])\n",
    "test_set_numerical_st = standard_scaler.transform(test_set[cols])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the `standard_scaler` object we use its method `fit_transform` **only in the numerical part of the `train_set`**. Then, **for test set, we use the `transform` method.** This is done to avoid any information leakage from the train to the test set, since it will be used to evaluate the machine learning models. In addition, since we assume that both the train and the test sets come from the same distribution, then there should be no difference between their means and standard deviations.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both these methods return *numpy arrays*. We will create two copies of the `train_set` and `test_set` dataframes and modify these copies to hold the standardized values instead of the original ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_stand = train_set.copy()\n",
    "train_set_stand[cols] = train_set_numerical_st\n",
    "train_set_stand['split'] = 'train'\n",
    "\n",
    "test_set_stand = test_set.copy()\n",
    "test_set_stand[cols] = test_set_numerical_st\n",
    "test_set_stand['split'] = 'test'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also added a new column two both the train and test dataframes to indicate the split that the samples belong to."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a look how standardization changes the distribution of the features. For  this, we will plot the histogram of the values for the `fruit` category and the `energy_kcal` feature. Note that this step is not necessarily part of the machine learning pipeline we have seen so far. This is not needed for the model itself, but we will use it to give us an idea of what happens after standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the whole train_set select only samples of the 'fruits' category\n",
    "# for all samples of the 'fruit' category select only the `energy_kcal` column\n",
    "original_fruit_energy = train_set[train_set['category']=='fruits']['energy_kcal']\n",
    "\n",
    "# from the whole train_set_stand select only samples of the 'fruits' category\n",
    "# for all samples of the 'fruit' category select only the `energy_kcal` column\n",
    "standardized_fruit_energy = train_set_stand[train_set_stand['category']=='fruits']['energy_kcal']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by selecting the `energy_kcal` features for all the samples in both the original and standardized train sets and see how the mean and standard deviation change after standardization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    867.000000\n",
       "mean     208.689735\n",
       "std      174.316367\n",
       "min        0.000000\n",
       "25%       76.000000\n",
       "50%      155.000000\n",
       "75%      313.000000\n",
       "max      900.000000\n",
       "Name: energy_kcal, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set['energy_kcal'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    8.670000e+02\n",
       "mean     3.892824e-17\n",
       "std      1.000577e+00\n",
       "min     -1.197881e+00\n",
       "25%     -7.616400e-01\n",
       "50%     -3.081795e-01\n",
       "75%      5.987417e-01\n",
       "max      3.968126e+00\n",
       "Name: energy_kcal, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_stand['energy_kcal'].describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `describe()` method of the pandas dataframe to get some statistics about the distribution of the values in the `energy_kcal` column."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By analyzing these statistics, we can see that after standardization the data has become more uniform. This is evident when we see the mean and standard deviation before and after standardization. The mean before standardization was $206.24$, while the mean after standardization is $-7.38 \\times 10^{-17}$. The standard deviation has also changed. Before the standardization it was $169.21$ while after the standardization it is $1.0$, so the range of values is smaller and therefore the distribution of values more uniform."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to concatenate the separate `train_set_stand` and `test_set_stand` dataframes in a single standardized dataset and save it in a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the standardized train and test dataframes\n",
    "# sort them using the ID/index column\n",
    "dataset_stand = pd.concat([train_set_stand, test_set_stand]).sort_index()\n",
    "dataset_stand\n",
    "\n",
    "# save the standardized dataframe\n",
    "dataset_stand.to_csv('data/swiss_food_composition_proc.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having processed our food composition dataset, we can now apply supervised and unsupervised ML methods to it in the next two modules."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ **CHECKPOINT:**\n",
    "- Why is standardization important?\n",
    "- Why `fit_transform()` is applied on the train set and `transform()` on the test set?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    " - \"Machine Learning with Pytorch and Scikit-Learn\" - Sebastian Raschka, Yuxi Liu, Vahid Mirjalili, Dmytro Dzhulgakov.\n",
    " - Raw data from https://naehrwertdaten.ch/en/downloads/."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
