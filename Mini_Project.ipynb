{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module we are going to put everything we studied so far together. We will build a pipeline that contains both supervised and unsupervised tasks. Using again the Swiss Food Composition dataset, we will train a model that will be able to distinguish between `cereals` and `other` food categories, using the dataset with all the features and then the dimensionality reduced dataset.\n",
    "\n",
    "[Figure 1](#ml_pipeline) gives an illustration of the pipeline in both cases.\n",
    "\n",
    "<center>\n",
    "    <a id=\"ml_pipeline\"></a>\n",
    "    <img src=\"images/mini_project/ml_pipeline_in_project.jpg\" alt=\"ML Pipeline\" width=\"80%\" height=\"60%\">\n",
    "    <center><figcaption><em>Figure 1: ML Pipeline Illustration</em></figcaption></center>\n",
    "</center>\n",
    "\n",
    "In the code snippets below, we are going to train a `RandomForestClassifier`, first in the dataset with all the features and then in the dimensionality reduced dataset using `UMAP`.\n",
    "\n",
    "**Overview:**\n",
    "1. [Read the data](#read_data)\n",
    "2. [Defining Helper Functions](#helper_functions)\n",
    "3. [Model without Dimensionality Reduction](#model_without_dim)\n",
    "4. [Model with Dimensionality Reduction](#model_with_dim)\n",
    "5. [Questions](#questions)\n",
    "\n",
    "Note that it suffices to have a high level understanding of what the helper functions do by reading their documentation and not going in too much of detail. You do not need to change anything related to these functions. We already indicate the code parts that should be changed so that you can answer the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"read_data\"></a>\n",
    "## 1. Read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we will start by importing the necessary libraries, defining some helper functions and reading the data. We will use the processed version of the Swiss Food Composition Dataset that we used in the previous modules (`data/swiss_food_composition_proc.csv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general packages\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# packages for performance evaluation\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "# packages for classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import  UMAP\n",
    "\n",
    "# we need these packages for visualizing the results\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we read the dataset and store it in the `dataset` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will work with the preprocessed dataset\n",
    "filepath = os.path.join(\"data\", \"swiss_food_composition_proc.csv\")\n",
    "dataset = pd.read_csv(filepath, index_col='ID')\n",
    "# save the numerical columns in the numerical_cols variable\n",
    "numerical_cols = dataset.select_dtypes(include='number').columns\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"helper_functions\"></a>\n",
    "## 2. Define helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to define some helper functions that we will need later for the training and evaluation of the models. You do not need to understand what these functions do to complete the project. Also you should not change anything within these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_sets(dataset:pd.DataFrame, feature_cols:list, target_col:str):\n",
    "    '''\n",
    "    Function used to split the dataset into train and test sets and return the train\n",
    "    and test features and target variables. The split is based on the pre-processing steps \n",
    "    included in the clean_chocolate_dataset.py script.\n",
    "\n",
    "    Args:\n",
    "     - dataset: the dataset that will be split\n",
    "     - feature_cols: a list with the names of the feature columns\n",
    "     - target_col: a string with the name of the target column\n",
    "\n",
    "    Returns:\n",
    "     - four pandas dataframes containing the train features, train targets,\n",
    "     test features and test targets.\n",
    "    '''\n",
    "\n",
    "    train_data = dataset[dataset['split'] == 'train']\n",
    "    test_data = dataset[dataset['split'] == 'test']\n",
    "    \n",
    "    return train_data[feature_cols], train_data[target_col], \\\n",
    "        test_data[feature_cols], test_data[target_col]\n",
    "\n",
    "def encode_label_category(label:str, label_column:pd.Series):\n",
    "    '''\n",
    "    This function converts the label_column to a column containing binary values\n",
    "    of 0 or 1. The rows whose value will be equal to `label` value will contain a 1. All\n",
    "    the other rows will contain a 0.\n",
    "\n",
    "    Args:\n",
    "        - label: the category that will be encoded with 1\n",
    "        - label_column: the column that will be transformed\n",
    "\n",
    "    Return:\n",
    "    The transformed column.\n",
    "    '''\n",
    "    return label_column.apply(lambda x: 1 if x == label else 0)\n",
    "\n",
    "def prepare_dataset(label:str, original_dataset:pd.DataFrame):\n",
    "    \n",
    "    ''' \n",
    "    This function is used to prepare the original dataset for the next steps in \n",
    "    supervised learning. It extracts the labels, does the manual encoding of the \n",
    "    category specified in the label parameter and splits the dataset into train and \n",
    "    test based on the split from Module 1. Also, here the features and labels are\n",
    "    splitted in different dataframes.\n",
    "\n",
    "    Args: \n",
    "        - label: the category that will be classified\n",
    "        - original_datatset: the dataset\n",
    "    \n",
    "    Returns:\n",
    "        - train_features, test_features, train_labels, test_labels\n",
    "    '''\n",
    "    \n",
    "    labels_col = original_dataset['category']\n",
    "    \n",
    "    one_hot_labels = encode_label_category(label=label, label_column=labels_col)\n",
    "    \n",
    "    original_dataset['category_binary'] = one_hot_labels\n",
    "    \n",
    "    not_needed_cols = ['name', 'category', 'split', 'category_binary']\n",
    "    feature_cols = original_dataset.columns.difference(not_needed_cols)\n",
    "    target_col = 'category_binary'\n",
    "\n",
    "    return get_train_test_sets(original_dataset, feature_cols=feature_cols,\\\n",
    "                                target_col=target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_auc_roc(clf, X_train:pd.DataFrame, X_test:pd.DataFrame, \n",
    "                    y_train:pd.DataFrame, y_test:pd.DataFrame, sup_title:str, \n",
    "                    dim_red:str):\n",
    "\n",
    "    '''\n",
    "    This function is used to visualize the ROC curve, as well as to\n",
    "    calculate and display the accuracy and F1 scores. \n",
    "\n",
    "    Args:\n",
    "     - clf: the classifier\n",
    "     - X_train: is the features of the train set\n",
    "     - X_test: is the features of the test set\n",
    "     - y_train: the labels of the train set\n",
    "     - y_test: the labels of the test set\n",
    "     - sup_title: the title of the plot\n",
    "     - dim_red: is the variable that indicates if/what\n",
    "     dimensionality reduction technique is used\n",
    "\n",
    "    Returns: nothing\n",
    "    '''\n",
    "\n",
    "    roc_title = f'ROC - curve'\n",
    "      \n",
    "    print(f\"Accuracy (Test): {accuracy_score(y_true=y_test, y_pred=clf.predict(X_test)):.2f}\")\n",
    "    print(f\"F1-score (Test): {f1_score(y_true=y_test, y_pred=clf.predict(X_test)):.2f}\")\n",
    "    print(f\"Accuracy (Train): {accuracy_score(y_true=y_train, y_pred=clf.predict(X_train)):.2f}\")\n",
    "    print(f\"F1-score (Train): {f1_score(y_true=y_train, y_pred=clf.predict(X_train)):.2f}\")\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 4))\n",
    "    ax.set_title(roc_title)\n",
    "    RocCurveDisplay.from_estimator(clf, X_test, y_test, \\\n",
    "        name=\"Test set\", ax=ax, alpha=0.8, plot_chance_level=True)\n",
    "    RocCurveDisplay.from_estimator(clf, X_train, y_train, \\\n",
    "        name=\"Train set\", ax=ax, alpha=0.8)\n",
    "\n",
    "    img_filepath = os.path.join(\"images\", \"plots\",f'{sup_title.replace(\" \", \"_\")}_{dim_red}')\n",
    "    \n",
    "    plt.suptitle(sup_title)\n",
    "    plt.show()\n",
    "    # save the figure in the plots folder\n",
    "    fig.savefig(img_filepath, bbox_inches='tight')\n",
    "    print(f'Plot saved in {img_filepath}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifier(classifier, clf_type:str, dataset:pd.DataFrame, dim_red:str=\"no_dim_red\"):\n",
    "    '''\n",
    "    This function splits the dataset into the train and test sets. Then it trains the \n",
    "    classifier model. Afterwards the results are visualized by calling the \n",
    "    run_auc_roc() helper function.\n",
    "\n",
    "    Args:\n",
    "     - classifier: the model that will be trained and evaluated\n",
    "     - clf_type: the type of classifier\n",
    "     - dataset: a pandas dataframe containing the dataset\n",
    "     - dim_red: is the variable that indicates what dimensionality\n",
    "     reduction technique is used\n",
    "    \n",
    "    Returns: nothing\n",
    "    '''\n",
    "    # split the dataset into train and test set\n",
    "    train_data_features, train_data_labels,\\\n",
    "        test_data_features, test_data_labels \\\n",
    "            = prepare_dataset(label=\"cereals\", original_dataset=dataset)\n",
    "    \n",
    "    classifier.fit(train_data_features, train_data_labels.values.ravel())\n",
    "    \n",
    "    title=''\n",
    "    if clf_type==\"lr\":\n",
    "        title = 'Logistic Regression'\n",
    "    elif clf_type==\"rf\":\n",
    "        title = 'Random Forest'\n",
    "    \n",
    "    # visualize the results\n",
    "    run_auc_roc(classifier, X_train=train_data_features, \n",
    "                                   y_train=train_data_labels, X_test=test_data_features,\n",
    "                                   y_test=test_data_labels, sup_title=title, dim_red=dim_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dimensionality_reduction(dimensionality_reducer, dataset:pd.DataFrame, \n",
    "                                 numerical_cols:list):\n",
    "    '''\n",
    "    This method creates the dimensionality reduced dataset using the fit_transform method. \n",
    "    The results of dimensionality reduction are saved in a pandas dataframe that \n",
    "    contain the 4 columns holding the new, reduced dimensions as well as the category \n",
    "    of the sample and the split (`train` or `test`). \n",
    "\n",
    "    Args:\n",
    "     - dimensionality_reducer: the object that will do the reduction\n",
    "     - dataset: a pandas dataframe containing the data\n",
    "     - numerical_cols: a list of the columns containing numerical data\n",
    "\n",
    "    Returns: a pandas dataframe representing the dataset in low dimensions\n",
    "    '''  \n",
    "    \n",
    "    low_dim_components = pd.DataFrame(\n",
    "                        data=dimensionality_reducer.fit_transform(dataset[numerical_cols]), \n",
    "                        columns=['dim_1', 'dim_2'],\n",
    "                        index=dataset.index #this is important for the concatenation in the next command\n",
    "                        )\n",
    "\n",
    "    data_after_dim_red = pd.concat([low_dim_components, dataset['category'], dataset['split']], axis=1)\n",
    "    data_after_dim_red.columns = ['dim_1', 'dim_2', 'category', 'split']\n",
    "\n",
    "    return data_after_dim_red   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model_without_dim\"></a>\n",
    "## 3. Model without dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having loaded the dataset and defined the helper functions we are ready to initialize, train and evaluate a model's performance. As an example, we will train a `RandomForestClassifer` with 30 trees, each of which has at most 4 levels.\n",
    "\n",
    "After that, we are ready to train the model. The actual training happens inside the `run_classifier` helper function. Below you can see the performance metrics of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier hyperparameters\n",
    "n_estimators = 30\n",
    "max_depth = 4\n",
    "\n",
    "###### A. CODE TO BE CHANGED ######\n",
    "clf_type = 'rf' # change this to `lr` when using Logistic Regression \n",
    "\n",
    "# initialize the classifier here\n",
    "classifier = RandomForestClassifier(max_depth=max_depth,\\\n",
    "                                        n_estimators=n_estimators,\\\n",
    "                                            random_state=0)\n",
    "\n",
    "# train classifier and visualize results\n",
    "run_classifier(classifier=classifier, clf_type=clf_type, \\\n",
    "               dataset=dataset, dim_red=\"no_dim_red\")\n",
    "\n",
    "###### END CODE TO BE CHANGED ######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model_with_dim\"></a>\n",
    "## 4. Model with reduced features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scenario, we will use a setup similar to the previous one. However, there will be an extra step in the pipeline. Now the  model that we initialize is not trained on the dataset with all the features but instead on a dimensionality reduced dataset. Here we will use again a `RandomForestClassifier` with the same hyperparameters as above. We will use UMAP for dimensionality reduction. We will initialize the `umap` model with `n_neighbors=30` and `min_dist=0.5`. As usual, the number of components will be 2. By calling the `run_dimensionality_reduction` helper function we create the dimensionality reduced dataset. Having done this, we initialize the classifier and then train the model and visualize the results by calling the `run_classifier` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier hyperparameters\n",
    "n_estimators = 30\n",
    "max_depth = 4\n",
    "\n",
    "###### B. CODE TO BE CHANGED ######\n",
    "clf_type = 'rf' # change this to `lr` when using Logistic Regression \n",
    "\n",
    "# dimensionality reduction hyperparameters\n",
    "n_neighbors = 30\n",
    "min_dist = 0.5\n",
    "n_components = 2\n",
    "\n",
    "\n",
    "dim_red=\"umap\" # Change this to `tsne` when running\n",
    "\n",
    "# initialize dimensionality reduction technique\n",
    "umap = UMAP(n_components=2, n_neighbors=n_neighbors, \\\n",
    "                min_dist=min_dist, random_state=0)\n",
    "\n",
    "data_after_dimensionality_reduction = \\\n",
    "    run_dimensionality_reduction(umap, dataset=dataset, numerical_cols=numerical_cols)\n",
    "\n",
    "# initialize classifier\n",
    "classifier = RandomForestClassifier(max_depth=max_depth,\\\n",
    "                                        n_estimators=n_estimators,\\\n",
    "                                            random_state=0)\n",
    "\n",
    "# train classifier and visualize the results\n",
    "run_classifier(classifier=classifier, clf_type=clf_type, \\\n",
    "    dataset=data_after_dimensionality_reduction, dim_red=dim_red)\n",
    "\n",
    "###### END CODE TO BE CHANGED ######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"questions\"></a>\n",
    "## 5. Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1️⃣ Given the results obtained above in both scenarios, do you think the extra step of dimensionality reduction helps the model in learning the data better? What do the performance metrics indicate? Refer to the tables below for the summary of the results or you can check the metrics from the plots above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2️⃣ Now we need to test the performance of the `RandomForestClassifier` when the dimensionality technique used is `tSNE` with `perplexity=20` and `random_state=0`. For this you will need to change the code. Record the AUC ROC on the train and test sets in the new set up in the tables below.\n",
    "\n",
    "❗ **NOTE**: You will only need to initialize the `tSNE` object and use it instead of the `umap` object. There is only a single code cell where you need to make changes in the code. **DO NOT FORGET TO CHANGE THE VALUE OF THE `dim_red` VARIABLE TO `tsne`, OTHERWISE THE PLOTS WILL NOT BE SAVED.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3️⃣ Now we need to test the performance of the `LogisticRegression` classifier without dimensionality reduction. For this you will need to change the code when initializing the `RandomForestClassifier`. No hyperparameters are needed in this case. Record the AUC ROC on the train and test sets in the new set up in the tables below.\n",
    "\n",
    "❗ **NOTE**: You will only need to initialize the `LogisticRegression` object and pass it to the `run_classifier` function. There is only a single code cell where you need to make changes in the code. **DO NOT FORGET TO CHANGE THE `clf_type='lr'`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4️⃣ Now we need to test the performance of the `LogisticRegression` classifier when the  dimensionality reduction technique used is `tSNE`. The `perplexity` should be set to 20 again. Record the AUC ROC on the train and test sets in the new set up in the tables below.\n",
    "\n",
    "❗ **NOTE**: You will only need to initialize the `LogisticRegression` object and pass it to the `run_classifier` function. There is only a single code cell where you need to make changes in the code. **DO NOT FORGET TO CHANGE THE `clf_type='lr'`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5️⃣ As the final step, we need to test the performance of the `LogisticRegression` classifier when the  dimensionality reduction technique used is `UMAP` with `n_neighbors=30`, `min_dist=0.5`, `random_state=0` and `n_components=2`. Record the AUC ROC on the train and test sets in the new set up in the tables below.\n",
    "\n",
    "❗ **NOTE**: In case you have followed the steps accordingly so far, you will only need to initialize the `UMAP` object and use it instead of the `tSNE` object. There is only a single code cell where you need to make changes in the code. **DO NOT FORGET TO CHANGE THE VALUE OF THE `dim_red` VARIABLE TO `umap`, OTHERWISE THE PLOTS WILL NOT BE SAVED.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Train set AUC ROC | No dim. red. | UMAP | tSNE |\n",
    "| -------- | -------- | -------- | -------- |\n",
    "| Logistic Regression   | <!--AUC for plain logistic regression-->   | <!--AUC for logistic regression and UMAP-->   | <!--AUC for logistic regression and tSNE-->   |\n",
    "| Random Forest   | 1.0   | 0.99   | <!--AUC for random forest and tSNE-->   |\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "| Test set AUC ROC | No dim. red. | UMAP | tSNE |\n",
    "| -------- | -------- | -------- | -------- |\n",
    "| Logistic Regression   | <!--AUC for plain logistic regression-->   | <!--AUC for logistic regression and UMAP-->   | <!--AUC for logistic regression and tSNE-->   |\n",
    "| Random Forest   | 0.98   | 0.98  | <!--AUC for random forest and tSNE-->   |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6️⃣ After having collected the results in the above table, which model do you think has the best performance? Why? Are there any indications of overfitting or underfitting? You can find all the plots saved in the `/images/plots/` folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
