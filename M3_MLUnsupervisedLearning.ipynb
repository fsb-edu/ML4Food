{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the third module of this series! In this module you will first get a deeper look on what Unsupervised Learning is and in which cases it can be used. Then you will explore one of the main use cases of unsupervised learning, namely dimensionality reduction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Module Overview**\n",
    "1. [Introduction to Unsupervised Learning](#what-is-unsupervised)\n",
    "2. [Dimensionality Reduction](#dim-red)<br>\n",
    "    2.1 [Why is it used?](#why-used)<br>\n",
    "    2.2 [PCA](#pca)<br>\n",
    "    2.3 [tSNE](#tsne)<br>\n",
    "    2.4 [UMAP](#umap)\n",
    "3. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**\n",
    "\n",
    "In this module we will work with the already preprocessed Swiss Food Composition dataset from **Module 1: Introduction to Machine Learning and Data Preprocessing for Food Sciences**. You can find the preprocessed dataset in the `data/swiss_food_composition_proc.csv`. \n",
    "\n",
    "As a quick recap, this is the resulting dataset after:\n",
    "- removing the samples and features with more than 20% of missing values,\n",
    "- splitting the dataset in train and test sets,\n",
    "- imputing missing values \n",
    "- standardizing the remaining data\n",
    "\n",
    "Note that in this module, we will not need the train and test splits since in the unsupervised \n",
    "learning case we do not make use of any labels or target variables and thus, we do not predict any category or value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='what-is-unsupervised'></a>\n",
    "## 1. Introduction to Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised machine learning is a type of machine learning that uses algorithms to analyze and cluster unlabelled datasets, thus the data that we have **does not have any values or categories that we can learn** and later predict, or we can simply decide to ignore the labels. Here, the models will try to find a structure in the data, or learn patterns present. Some use cases of such models would be: dimensionality reduction, clustering, data generation, anomaly detection, etc. \n",
    "\n",
    "In the case of [dimensionality reduction](#dim-red), we move from data with many features, to compressed data, with very few features. In the case of [clustering](https://scikit-learn.org/stable/modules/clustering.html), we try to find groups within the data, so that we can group similar samples together. While as the name suggests, in the case of [data generation](https://towardsdatascience.com/5-best-python-synthetic-data-generators-and-how-to-use-them-when-you-lack-data-f62bcf62d43c), we use the unlabelled data to learn a structure or underlying properties and based on this, the model will generate similar samples. For [anomaly detection](https://towardsdatascience.com/5-anomaly-detection-algorithms-every-data-scientist-should-know-b36c3605ea16), we can use machine learning models to find outliers in the data. Outliers are points that do not resemble the majority of the points in the dataset. \n",
    "\n",
    "[Fig. 1](#unsup_learn) illustrates the machine learning pipeline in case of unsupervised learning. Still there is an output from the model. It outputs what the model has \"learned\" from the data. In the case of dimensionality reduction, the output will be the samples with less features. As you can see from the figure, the new number of features `n'` is smaller than the original number of features `n`. \n",
    "\n",
    "Something to notice is the missing train-test split step. The purpose of the train-test split in supervised learning is to assess how well the model generalises to new, unseen data by evaluating its performance on a separate test set. Since unsupervised learning doesn't involve prediction or evaluation against target variables, the concept of a train-test split is not as applicable. However, there are specific cases where an unsupervised method can be indirectly evaluated. For example, in the case of dimensionality reduction, we can compare its impact on supervised learning tasks by comparing the performance of supervised learning models on the original and reduced datasets.\n",
    "\n",
    "<center>\n",
    "    <a id=\"unsup_learn\"></a>\n",
    "    <img src=\"images/part3_unsupervised/unsupervised_learning_dimred.jpg\" alt=\"ML Unsupervised Learning\" width=\"90%\">\n",
    "    <center><figcaption><em>Figure 1: Unsupervised Learning</em></figcaption></center>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ **CHECKPOINT:**\n",
    "- How does supervised learning differ from unsupervised learning? Think of some example use cases for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dim-red'></a>\n",
    "## 2. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='why-used'></a>\n",
    "###  2.1 Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High-dimensional datasets are datasets that have a lot of features compared to their number of samples. **Dimensionality reduction** is used to **summarize the information content of a high-dimensional dataset** by transforming it onto a new feature space of **lower dimensionality**. The dimensionality reduction algorithms identify the most important features that affect model performance, and transform the data in a way that would preserve most of the information. Besides that, they offer a lot of advantages especially when it comes to making results easier to visualize and explain to audiences, removing noise from the dataset, and making model training faster.\n",
    "\n",
    "The more features a datasets has, the more samples are needed so that the ML models can learn it. This is otherwise considered as **the curse of dimensionality**. Dimensionality reduction also enhances the performance of the ML models because it reduces the effects of the curse of dimensionality.  Therefore, one of the main goals of dimensionality reduction is to **reduce the number of features of the dataset while maintaining the most important information**.\n",
    "\n",
    "All dimensionality reduction techniques are part of the unsupervised learning group of \n",
    "algorithms. Some of these techniques include: principal component analysis (PCA), t-SNE (t-distributed stochastic embedding) and UMAP (uniform manifold approximation and projection). Below we will explore **PCA**, **t-SNE** and **UMAP** in the *processed Swiss Food Composition Dataset*. However, first, we will import the necessary libraries and read the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import  UMAP\n",
    "import pandas as pd\n",
    "\n",
    "# we need these packages for visualizing the results\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>energy_kcal</th>\n",
       "      <th>fat_g</th>\n",
       "      <th>fatty_acids_sat_g</th>\n",
       "      <th>fatty_acids_monounsat_g</th>\n",
       "      <th>fatty_acids_polyunsat_g</th>\n",
       "      <th>cholesterol_mg</th>\n",
       "      <th>carbohydrates_g</th>\n",
       "      <th>sugars_g</th>\n",
       "      <th>...</th>\n",
       "      <th>potassium_mg</th>\n",
       "      <th>sodium_mg</th>\n",
       "      <th>chloride_mg</th>\n",
       "      <th>calcium_mg</th>\n",
       "      <th>magnesium_mg</th>\n",
       "      <th>phosphorus_mg</th>\n",
       "      <th>iron_mg</th>\n",
       "      <th>iodide_Âµg</th>\n",
       "      <th>zinc_mg</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Almond</td>\n",
       "      <td>fruits</td>\n",
       "      <td>2.383884</td>\n",
       "      <td>2.367211</td>\n",
       "      <td>0.007568</td>\n",
       "      <td>3.747869</td>\n",
       "      <td>1.491113</td>\n",
       "      <td>-0.536758</td>\n",
       "      <td>-0.392082</td>\n",
       "      <td>0.004006</td>\n",
       "      <td>...</td>\n",
       "      <td>1.588591</td>\n",
       "      <td>-0.159777</td>\n",
       "      <td>-0.148160</td>\n",
       "      <td>1.295299</td>\n",
       "      <td>4.040213</td>\n",
       "      <td>2.312243</td>\n",
       "      <td>0.749351</td>\n",
       "      <td>-0.106408</td>\n",
       "      <td>1.315695</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Almond, dry roasted, salted</td>\n",
       "      <td>nuts</td>\n",
       "      <td>2.458504</td>\n",
       "      <td>2.390231</td>\n",
       "      <td>0.007568</td>\n",
       "      <td>3.980279</td>\n",
       "      <td>1.745907</td>\n",
       "      <td>-0.536758</td>\n",
       "      <td>-0.285661</td>\n",
       "      <td>-0.117252</td>\n",
       "      <td>...</td>\n",
       "      <td>1.488645</td>\n",
       "      <td>-0.083438</td>\n",
       "      <td>0.108003</td>\n",
       "      <td>1.295299</td>\n",
       "      <td>4.816868</td>\n",
       "      <td>2.059374</td>\n",
       "      <td>0.906205</td>\n",
       "      <td>-0.088162</td>\n",
       "      <td>1.315695</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Almond, roasted, salted</td>\n",
       "      <td>nuts</td>\n",
       "      <td>2.527384</td>\n",
       "      <td>2.545613</td>\n",
       "      <td>0.020843</td>\n",
       "      <td>4.212688</td>\n",
       "      <td>1.825530</td>\n",
       "      <td>-0.536758</td>\n",
       "      <td>-0.419844</td>\n",
       "      <td>-0.138650</td>\n",
       "      <td>...</td>\n",
       "      <td>1.355384</td>\n",
       "      <td>-0.050088</td>\n",
       "      <td>0.108003</td>\n",
       "      <td>1.098111</td>\n",
       "      <td>4.622704</td>\n",
       "      <td>2.059374</td>\n",
       "      <td>0.749351</td>\n",
       "      <td>-0.088162</td>\n",
       "      <td>1.185016</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Amaranth, seed, cooked (without addition of fa...</td>\n",
       "      <td>cereals</td>\n",
       "      <td>-0.514820</td>\n",
       "      <td>-0.510234</td>\n",
       "      <td>-0.483593</td>\n",
       "      <td>-0.476516</td>\n",
       "      <td>-0.196900</td>\n",
       "      <td>-0.536758</td>\n",
       "      <td>0.084497</td>\n",
       "      <td>-0.431096</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.377014</td>\n",
       "      <td>-0.159511</td>\n",
       "      <td>-0.149051</td>\n",
       "      <td>-0.098161</td>\n",
       "      <td>1.011260</td>\n",
       "      <td>0.226077</td>\n",
       "      <td>0.631711</td>\n",
       "      <td>-0.101432</td>\n",
       "      <td>-0.056440</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Amaranth, seed, raw</td>\n",
       "      <td>cereals</td>\n",
       "      <td>0.960362</td>\n",
       "      <td>-0.228244</td>\n",
       "      <td>-0.337572</td>\n",
       "      <td>-0.312462</td>\n",
       "      <td>0.121593</td>\n",
       "      <td>-0.536758</td>\n",
       "      <td>1.875138</td>\n",
       "      <td>-0.345502</td>\n",
       "      <td>...</td>\n",
       "      <td>0.722392</td>\n",
       "      <td>-0.158810</td>\n",
       "      <td>-0.132568</td>\n",
       "      <td>0.572278</td>\n",
       "      <td>4.234377</td>\n",
       "      <td>2.628328</td>\n",
       "      <td>2.984515</td>\n",
       "      <td>-0.087333</td>\n",
       "      <td>1.577054</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 name category  energy_kcal  \\\n",
       "ID                                                                            \n",
       "2                                              Almond   fruits     2.383884   \n",
       "3                         Almond, dry roasted, salted     nuts     2.458504   \n",
       "4                             Almond, roasted, salted     nuts     2.527384   \n",
       "5   Amaranth, seed, cooked (without addition of fa...  cereals    -0.514820   \n",
       "6                                 Amaranth, seed, raw  cereals     0.960362   \n",
       "\n",
       "       fat_g  fatty_acids_sat_g  fatty_acids_monounsat_g  \\\n",
       "ID                                                         \n",
       "2   2.367211           0.007568                 3.747869   \n",
       "3   2.390231           0.007568                 3.980279   \n",
       "4   2.545613           0.020843                 4.212688   \n",
       "5  -0.510234          -0.483593                -0.476516   \n",
       "6  -0.228244          -0.337572                -0.312462   \n",
       "\n",
       "    fatty_acids_polyunsat_g  cholesterol_mg  carbohydrates_g  sugars_g  ...  \\\n",
       "ID                                                                      ...   \n",
       "2                  1.491113       -0.536758        -0.392082  0.004006  ...   \n",
       "3                  1.745907       -0.536758        -0.285661 -0.117252  ...   \n",
       "4                  1.825530       -0.536758        -0.419844 -0.138650  ...   \n",
       "5                 -0.196900       -0.536758         0.084497 -0.431096  ...   \n",
       "6                  0.121593       -0.536758         1.875138 -0.345502  ...   \n",
       "\n",
       "    potassium_mg  sodium_mg  chloride_mg  calcium_mg  magnesium_mg  \\\n",
       "ID                                                                   \n",
       "2       1.588591  -0.159777    -0.148160    1.295299      4.040213   \n",
       "3       1.488645  -0.083438     0.108003    1.295299      4.816868   \n",
       "4       1.355384  -0.050088     0.108003    1.098111      4.622704   \n",
       "5      -0.377014  -0.159511    -0.149051   -0.098161      1.011260   \n",
       "6       0.722392  -0.158810    -0.132568    0.572278      4.234377   \n",
       "\n",
       "    phosphorus_mg   iron_mg  iodide_Âµg   zinc_mg  split  \n",
       "ID                                                       \n",
       "2        2.312243  0.749351  -0.106408  1.315695  train  \n",
       "3        2.059374  0.906205  -0.088162  1.315695  train  \n",
       "4        2.059374  0.749351  -0.088162  1.185016   test  \n",
       "5        0.226077  0.631711  -0.101432 -0.056440   test  \n",
       "6        2.628328  2.984515  -0.087333  1.577054  train  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will work with the preprocessed dataset\n",
    "filepath = os.path.join(\"data\", \"swiss_food_composition_proc.csv\")\n",
    "dataset = pd.read_csv(filepath, index_col='ID')\n",
    "# save the numerical columns in the cols variable\n",
    "numerical_cols = dataset.select_dtypes(include='number').columns\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define two helper functions that will visualize the data after applying the dimensionality reduction techniques. Note that we select only the first two dimensions of the reduced dataset (which are all the features the dataset will have after applying dimensionality reduction). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scatter(X_1: np.array, X_2: np.array, y: np.array,\n",
    "    selected_category: str, title: str):\n",
    "    \n",
    "    '''\n",
    "    This function will create the scatter plot of points in low dimensions.\n",
    "    It will display only the samples that belong to the `selected_category`.\n",
    "\n",
    "    Args:\n",
    "     - X_1: the first dimension of the data\n",
    "     - X_2: the second dimension of the data\n",
    "     - y: the category of the sample\n",
    "     - selected_category: the category selected by the user\n",
    "     - title: the title of the plot\n",
    "\n",
    "     Returns: nothing\n",
    "    '''\n",
    "\n",
    "    color_mapping = {cat: i for i, cat in enumerate(np.unique(y))}\n",
    "    \n",
    "    _, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    for category in np.unique(y):\n",
    "        category_mask = (y == category)\n",
    "        \n",
    "        if selected_category == \"All categories\":\n",
    "            ax.scatter(X_1[category_mask], X_2[category_mask], \\\n",
    "                c='C{}'.format(color_mapping[category]), label='{}'.format(category), s=8)\n",
    "        elif category == selected_category:\n",
    "            ax.scatter(X_1[category_mask], X_2[category_mask], \\\n",
    "                c='C{}'.format(color_mapping[category]), label='{}'.format(category), s=8)\n",
    "        else:\n",
    "            ax.scatter(X_1[category_mask], X_2[category_mask], \\\n",
    "                c='gray', alpha=0.2, s=8)\n",
    "    \n",
    "    ax.set_xlabel('dim_1')\n",
    "    ax.set_ylabel('dim_2')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    sns.move_legend(ax, \"center left\", bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n",
    "\n",
    "def visualize_data(data: pd.DataFrame, title: str):\n",
    "    '''\n",
    "    This method is used to initiate the process of data visualization.\n",
    "    It splits the dimensionality reduced data in two arrays one for each of the dimensions\n",
    "    and turns the categories in an array as well. Then it creates and displays a dropdown list,\n",
    "    where the user can control the category displayed in the plot. The actual visualization \n",
    "    happens in the create_scatter function.\n",
    "\n",
    "    Args:\n",
    "     - data: the dimensionality reduced data\n",
    "     - title: the title of the plot that will be displayed\n",
    "\n",
    "    Returns: nothing\n",
    "    \n",
    "    '''\n",
    "\n",
    "    sns.set(\n",
    "        rc={'figure.figsize':(7, 5), 'font.family': ['DejaVu Sans']},\n",
    "        style='white'\n",
    "    )\n",
    "\n",
    "    X_1 = np.array(data[['dim_1']])\n",
    "    X_2 = np.array(data[['dim_2']])\n",
    "    y = np.array(data[['category']])\n",
    "\n",
    "    category_dropdown = widgets.Dropdown(\n",
    "        options=[\"All categories\"] + list(np.unique(y)),\n",
    "        value=\"All categories\",\n",
    "        description='Category:'\n",
    "    )\n",
    "\n",
    "    output = widgets.Output()\n",
    "\n",
    "    def on_category_change(change):\n",
    "        selected_category = change['new']\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            create_scatter(X_1=X_1, X_2=X_2, y=y, title=title, \\\n",
    "                selected_category=selected_category)\n",
    "\n",
    "    category_dropdown.observe(on_category_change, names='value')\n",
    "\n",
    "    display(category_dropdown, output)\n",
    "    # this is for creating the initial plot\n",
    "    on_category_change({'new': category_dropdown.value})  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pca'></a>\n",
    "### 2.2 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Component Analysis** is a **linear dimensionality reduction technique**. It projects the data points to the directions of the highest variance, since they contain the most important information in the dataset. The data is projected into a new subspace with less features than before. **The new axes of the data are orthogonal to each other and are the directions of the maximum variance in the dataset.**\n",
    "\n",
    "[Fig. 2](#pca) gives an illustration of PCA:\n",
    "<center>\n",
    "    <a id=\"pca\"></a>\n",
    "    <img src=\"images/part3_unsupervised/PCA.jpg\" alt=\"PCA\" width=\"90%\">\n",
    "    <center><figcaption><em>Figure 2: PCA components</em></figcaption></center>\n",
    "</center>\n",
    "\n",
    "The left-hand side of the figure shows the original data in two dimensions. We find the directions of the highest variance and then project the data according to the subspace defined by these directions.\n",
    "The red axes in the middle figure depict the directions of the highest variance. The data points will be projected into these two directions, which is shown in the right-hand side of the figure. In this case there is no dimensionality reduction. The projected data points have two dimensions again, even after PCA, but they reside in a new subspace, defined by the directions of the highest variance. \n",
    "\n",
    "**NOTE**: In this figure, there is no dimensionality reduction, as both the original and the reduced datasets have two dimensions. Also, the reduced datasets do not necessarily always have 2 components. The number of components is a hyperparameter (you can refer to module 2 for the distinction between parameters and hyperparameters) set by the data analyst. Here we always choose 2 components to visualise the results and make the procedure more intuitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important concept related to dimensionality reduction is that of **variance explained**. In dimensionality reduction, variance explained stands for the amount of information retained when the original high-dimensional data is projected into a lower-dimensional space. In case of PCA, it orders the components in increasing order of the amount of variance they explain. The first principal component explains the largest amount of variance in the data, the second component explains the second-largest amount of variance, and so on. By summing up the amounts of variance of the first *n* principal components we choose to project the data, we can quantify how much information is retained by reducing the dataset to *n* dimensions using PCA. Usually, the explained variance is reported as a percentage. For example, if the first three principal components of a dataset explain 60% of the total variance, it means that those three components capture 60% of the information present in the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA is sensitive to data scaling.** If variables in the dataset have different units of measurement, this can lead to biased results and inaccurate interpretations of the principal components. Therefore, it is important that the features are scaled before inputted to the PCA algorithm, if the features of the data are measured on different scales. Our dataset is already standardized from the first tutorial. So we are ready to apply PCA and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2) # define the PCA object\n",
    "\n",
    "# perform the PCA transformation and save the results in a dataframe\n",
    "pca_components = pd.DataFrame(\n",
    "    pca.fit_transform(dataset[numerical_cols]), # apply PCA here \n",
    "    columns=['dim_1', 'dim_2'], # define the new column names\n",
    "    index=dataset.index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a PCA object that will perform the transformation. We specify the number of reduced dimensions, which in our case will be 2 (`n_components=2`). Then we apply the PCA transformation to the numerical part of our dataset by using the `fit_transform()` method of the `pca` object (`pca.fit_transform(dataset[numerical_cols])`). Note that this method takes the original dataset and reduces the 38 numerical features to 2 only. We save the new features in a dataframe. This dataframe will contain two columns `dim_1` and `dim_2`, one for each of the dimensions after PCA (`columns=['dim_1', 'dim_2']`). Finally, in order for the reduced dataset to have the same index as the original  dataset, we set the index to the dataset index (`index=dataset.index`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.581815</td>\n",
       "      <td>-6.828131</td>\n",
       "      <td>fruits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.037107</td>\n",
       "      <td>-7.033969</td>\n",
       "      <td>nuts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.362831</td>\n",
       "      <td>-7.156481</td>\n",
       "      <td>nuts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.735546</td>\n",
       "      <td>0.174700</td>\n",
       "      <td>cereals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.008247</td>\n",
       "      <td>-4.400579</td>\n",
       "      <td>cereals</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dim_1     dim_2 category\n",
       "ID                             \n",
       "2   3.581815 -6.828131   fruits\n",
       "3   4.037107 -7.033969     nuts\n",
       "4   3.362831 -7.156481     nuts\n",
       "5  -0.735546  0.174700  cereals\n",
       "6   3.008247 -4.400579  cereals"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recreate the dataset but with the pca_components\n",
    "data_after_pca = pd.concat([pca_components, dataset['category']], axis=1)\n",
    "data_after_pca.columns = ['dim_1', 'dim_2', 'category'] # rename the columns\n",
    "data_after_pca.head() # print the first 5 samples after PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the reduced dataset, we concatenate the `pca_components` with the *category* column of the dataset and form a new dataframe. The concatenation is horizontal (`axis=1`), meaning that the *category* is appended to  the right of the `pca_components` as a new column. We define the new columns of the dataframe to be `dim_1`, `dim_2` and `category`. You can see the first 5 samples as the output of the `head()` column. As you can see, the dataset now contains two numerical  features and the category.\n",
    "\n",
    "After this, we can visualize the data by using our method defined in the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df4a554730ae481693f5e981bb2ff0f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Category:', options=('All categories', 'alcoholic_beverages', 'cereals', 'dairy', 'fruitâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166dfc4b7fc9448d87125158a500012d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_data(data_after_pca, 'PCA embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see where different food categories reside in the 2D space after applying PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find more information regarding PCA in [Chapter 5 of Machine Learning with Pytorch and Scikit-Learn](https://learning.oreilly.com/library/view/machine-learning-with/9781801819312/Text/Chapter_5.xhtml#_idParaDest-94) book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ **CHECKPOINT:**\n",
    "- Why do you think the samples from different categories are clustered together (categories are not well separated) after PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tsne'></a>\n",
    "### 2.3 tSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**t-distributed Stochastic Neighbor Embedding (t-SNE)** is a **non-linear** dimensionality reduction technique. It is mainly used for visualizing high-dimensional datasets in low dimensions. tSNE is useful in datasets where groups/clusters are not linearly separable. Usually, in these circumstances, linear dimensionality reduction techniques like PCA do not work well.\n",
    "tSNE projects data points into the low dimensional space by trying to preserve the distances they have in the high-dimensional space. Also, tSNE requires the whole dataset to project it to the low-dimensional space and cannot be applied to new data points directly, without recalculating the models. You can consult [the original paper](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) for more information regarding how tSNE works. Alternatively, you can read [this blog post](https://towardsdatascience.com/t-sne-clearly-explained-d84c537f53a) for a more intuitive explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will proceed with the implementation of tSNE. We will use the sklearn implementation of tSNE to reduce the original dataset to 2 dimensions for visualizing it, just like we did with PCA in the previous section.\n",
    "\n",
    "In order to  make tSNE interactive, we define a function (`run_tsne`) that receives the **perplexity** as argument and uses tSNE to reduce the dataset to 2 dimensions. Perplexity is one of the hyperparameters of tSNE that controls the preservation of  local and global structure  of the data points. It determines the number of neighboring points that the algorithm checks for each point when determining the similarity between points in high-dimensional space. The larger the value of perplexity, the more neighbours will be considered for each data point. Large perplexity values favour preservation of global structure. On the other hand, low perplexity values favour preservation of local structure. Here we provide a slider to experiment with its different values and see the effects it has in the new dataset. You can check different values of the perplexity hyperparameter ranging from 10 to 100, but in practice typical values of perplexity are between 5 and 50. To learn more about the hyperparameters of tSNE you can refer to [this article](https://distill.pub/2016/misread-tsne/).\n",
    "\n",
    "Apart from this, the code that performs the dimensionality reduction is similar to that of PCA. First we define a `tsne` object that will perform the dimensionality reduction. It will reduce the original dataset to 2 dimensions. It is initialized with the following hyperparameters:\n",
    " - `n_components` - the number of features of the reduced dataset\n",
    " - `init='pca'` to initialize the tSNE embedding, recommended in the original paper. \n",
    " - `perplexity` - determined by the slider. \n",
    " - `random_state=0` - to make the results deterministic\n",
    " \n",
    " After the `tsne` object is initialized, we use it to perform the dimensionality reduction. By using the `fit_transform()` method with the numerical features of the original dataset only, we get the reduced version with 2 features, named as `dim_1` and `dim_2`. We save the reduced dataset in a pandas dataframe called `tsne_components`. Then, we create a dataframe that will contain the two features of the dataset after applying tSNE and the `category` column of the original dataset. In the end, we use our method `visualize_data` to visualize the new dataset after reducing its dimensions using tSNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tsne(perplexity:int, dataset:pd.DataFrame, numerical_cols: list):\n",
    "    '''\n",
    "    This method is used to create the TSNE object that will perform the \n",
    "    dimensionality reduction using the fit_transform method. The results of dimensionality\n",
    "    reduction are saved in a pandas dataframe that contain the 2 columns holding\n",
    "    the new, reduced dimensions as well as a third column for the category of the \n",
    "    sample. \n",
    "\n",
    "    Args:\n",
    "     - perplexity: the perplexity parameter of TSNE, that will control the preservation\n",
    "     of local structure.\n",
    "     - dataset: a pandas dataframe containing the data\n",
    "     - numerical_cols: a list of the columns containing numerical data\n",
    "\n",
    "    Returns: nothing\n",
    "    '''\n",
    "    tsne = TSNE(n_components=2, init='pca', perplexity=perplexity, random_state=0)\n",
    "\n",
    "    tsne_components = pd.DataFrame(\n",
    "        data=tsne.fit_transform(dataset[numerical_cols]), \n",
    "        columns=['dim_1', 'dim_2'],\n",
    "        index=dataset.index #this is important for the concatenation in the next command\n",
    "        )\n",
    "    \n",
    "    data_after_tsne = pd.concat([tsne_components, dataset['category']], axis=1)\n",
    "    data_after_tsne.columns = ['dim_1', 'dim_2', 'category']\n",
    "    \n",
    "    visualize_data(data_after_tsne, \n",
    "                   f'tSNE embeddings - perplexity={perplexity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a7e932e57f4dec8b76c4e61b5f87db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=30, description='Perplexity:', min=10, step=5), Button(description='Run â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity_slider = widgets.IntSlider(value=30, min=10, max=100, step=5, description='Perplexity:')\n",
    "widgets.interact_manual(run_tsne, perplexity=perplexity_slider, \\\n",
    "                        dataset = widgets.fixed(dataset), \\\n",
    "                        numerical_cols = widgets.fixed(numerical_cols));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this cell to experiment more with the hyperparameters and compare \n",
    "# the plots. You will need to uncomment the below lines.\n",
    "\n",
    "# perplexity_slider = widgets.IntSlider(value=30, min=10, max=100, step=5, description='Perplexity:')\n",
    "# widgets.interact_manual(run_tsne, perplexity=perplexity_slider);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ **CHECKPOINT:**\n",
    "- Experiment with different values of perplexity. What do you notice? How do different values of perplexity affect the grouping of different categories? At what values of perplexity do the samples of each category cluster together?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='umap'></a>\n",
    "### 2.4 UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uniform Manifold Approximation and Projection (UMAP)** is another dimensionality reduction technique that we will explore. Just like tSNE, it is **non-linear**, however, unlike tSNE it tries to preserve both **the global structure of the dataset** and **the inter-cluster distances**. In addition, UMAP works better than tSNE with large datasets. Just like tSNE, UMAP also requires the whole dataset to project it to the low-dimensional space and cannot be applied to new data points directly, without recalculating the model. For more details on the theory of UMAP, you can read [this blog post](https://pair-code.github.io/understanding-umap/).\n",
    "\n",
    "Now we will start with the implementation of UMAP. The code is similar to that of tSNE, however here we work with two hyperparameters: `n_neighbors` and `min_dist`. `n_neighbors` is the most important hyperparameter of UMAP. It controls how UMAP balances the local and global structures. UMAP will put more effort in preserving local structure when the `n_neighbors` is low, since the number of points considered in high dimensions for each single point will be low (only the nearest neighbors). `min_dist` on the other hand, controls the minimum distance between points in the low-dimensional space. The lowest the value, the closer are the points in the low-dimensional space. We provide to sliders, one for the `n_neighbors` with values ranging from 10 to 100 and one for the `min_dist` with values ranging from 0 to 1. Note that in order for the visualization to appear, you must click the *Run Interact* button.\n",
    "\n",
    "\n",
    "To reduce the dimensions of the original dataset, we use the `run_umap` function that takes as arguments the `n_neighbors` and the `min_dist` from the sliders. It will create the `umap` object, with the following hyperparameters:\n",
    " - `n_components=2` - the number of features of the reduced dataset\n",
    " - `min_dist` and `n_neighbors` - determined by the slider values\n",
    " - `random_state=0` - a seed to make results deterministic and reproducible for the same dataset and set of hyperparameters\n",
    "\n",
    "After the `umap` object is initialized, we use it to perform the dimensionality reduction. By using the `fit_transform()` method with the numerical features of the original dataset only, we get the reduced version with only 2 features, names as `dim_1` and `dim_2`. We save the reduced dataset in a pandas dataframe called `umap_components`. We specify the index of this dataframe to be the same as the index of the original dataframe (`index=dataset.index`), as well. Then, we concatenate the reduced dataset with the `category` column of the original dataset and save it in a new dataframe (`data_after_umap`). We use this dataframe to visualize the results by calling the `visualize_data()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_umap(min_dist:float, n_neighbors:int, dataset:pd.DataFrame, numerical_cols:list):\n",
    "    '''\n",
    "    This method is used to create the UMAP object that will perform the \n",
    "    dimensionality reduction using the fit_transform method. The results of dimensionality\n",
    "    reduction are saved in a pandas dataframe that contain the 2 columns holding\n",
    "    the new, reduced dimensions as well as a third column for the category of the \n",
    "    sample. \n",
    "\n",
    "    Args:\n",
    "     - min_dist: UMAP hyperparameter\n",
    "     - n_neighbors: UMAP hyperparameter\n",
    "     - dataset: a pandas dataframe containing the data\n",
    "     - numerical_cols: a list of the columns containing numerical data\n",
    "\n",
    "    Returns: nothing\n",
    "    '''  \n",
    "    \n",
    "    umap = UMAP(n_components=2, n_neighbors=n_neighbors, \n",
    "                min_dist=min_dist, random_state=0)\n",
    "    \n",
    "    umap_components = pd.DataFrame(\n",
    "                        data=umap.fit_transform(dataset[numerical_cols]), \n",
    "                        columns=['dim_1', 'dim_2'],\n",
    "                        index=dataset.index #this is important for the concatenation in the next command\n",
    "                        )\n",
    "\n",
    "    data_after_umap = pd.concat([umap_components, dataset['category']], axis=1)\n",
    "    data_after_umap.columns = ['dim_1', 'dim_2', 'category']\n",
    "\n",
    "    visualize_data_2(data_after_umap, \n",
    "                   f'UMAP embeddings - n_neighbors={n_neighbors} and min_dist={min_dist}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e999b55ec1a944b88b8b7218d57cbb7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.5, description='min_dist:', max=1.0, step=0.05), IntSlider(value=30,â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_neighbors_slider = widgets.IntSlider(value=30, min=10, max=100, step=5, description='n_neighbors:')\n",
    "min_dist_slider = widgets.FloatSlider(value=0.5, min=0, max=1, step=0.05, description='min_dist:')\n",
    "widgets.interact_manual(run_umap, min_dist=min_dist_slider, n_neighbors=n_neighbors_slider, \\\n",
    "                        dataset = widgets.fixed(dataset), numerical_cols = widgets.fixed(numerical_cols));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this cell to experiment more with the hyperparameters and compare \n",
    "# the plots. You will need to uncomment the below lines.\n",
    "\n",
    "# n_neighbors_slider = widgets.IntSlider(value=30, min=10, max=100, step=5, description='n_neighbors:')\n",
    "# min_dist_slider = widgets.FloatSlider(value=0.5, min=0, max=1, step=0.05, description='min_dist:')\n",
    "# widgets.interact_manual(run_umap, min_dist=min_dist_slider, n_neighbors=n_neighbors_slider, dataset = widgets.fixed(dataset));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ **CHECKPOINT:**\n",
    "- Experiment with different values of `n_neighbors` and `min_dist`. What do you notice? How do different values of these hyperparameters affect the grouping of different categories? At what values do the samples of each category cluster together? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "## 3. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| | PCA | tSNE | UMAP |\n",
    "| --------------- | --------------- | --------------- | --------------- |\n",
    "| **objective**    | maximize variance    | preserve local structure    | preserve local and global structure    |\n",
    "| **linearity**    | linear    | non-linear    | non-linear    |\n",
    "| **scalability**    | fast for large datasets    | the slowest for large datasets    | slow for large datasets    |\n",
    "| **most important parameters**    | `n_components`    | `n_components`, `perplexity`    | `n_components`, `n_neighbors`, `min_dist`    |\n",
    "| **support for new data points**    | Yes    | Not directly    | Not directly    |\n",
    "\n",
    "</div>\n",
    "\n",
    "In this tutorial, we dived in the unsupervised learning part of machine learning. First, we explored what unsupervised learning is. Then, we explained what dimensionality reduction is. Afterwards, we saw 3 different dimensionality reduction techniques: PCA, tSNE and UMAP. The table above summarizes the main characteristics of each of these three dimensionality reduction techniques.  As we already saw, the focus of PCA is to maximize the preserved variance after reducing the dimensions of the dataset, while tSNE and UMAP focus on preserving local and global structure. PCA is a linear technique, while tSNE and UMAP are non-linear. In addition, PCA is the fastest of them, tSNE is the slowest for large datasets and UMAP is faster than tSNE, but still slower than PCA. In terms of hyperparameters tuned, in all three methods we determined the `n_components`, which determined the number of features in the lower dimensional space. tSNE had an additional parameter, `perplexity` that controlled the trade off between preserving local and global structure, while in UMAP this was done by two hyperparameters: `min_dist` and `n_neighbors`. Finally, only PCA can be applied to new points (e.g calculated on train set and applied to samples of test set), while for tSNE and UMAP, the model needs to be recalculated when new points are added. \n",
    "\n",
    "This concludes, this module and this series on Machine Learning for Food Science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"Machine Learning with Pytorch and Scikit-Learn\" - Sebastian Raschka, Yuxi Liu, Vahid Mirjalili, Dmytro Dzhulgakov.\n",
    "- [\"tSNE clearly explained\"](https://towardsdatascience.com/t-sne-clearly-explained-d84c537f53a) - Kemal Erdem\n",
    "- [\"Understanding UMAP\"](https://pair-code.github.io/understanding-umap/) - Andy Coenen, Adam Pearce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210d0aaceb4b4cee83cfd7aaf5ed1fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Checkbox(value=False, description='Option 1'), Checkbox(value=False, descriptionâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create Checkbox widgets\n",
    "checkboxes = [widgets.Checkbox(value=False, description=f'Option {i+1}') for i in range(12)]\n",
    "\n",
    "# Organize the checkboxes in rows of 4\n",
    "checkbox_rows = [widgets.HBox(checkboxes[i:i+4]) for i in range(0, 12, 4)]\n",
    "\n",
    "# Create a VBox layout to display the checkbox rows vertically\n",
    "checkboxes_vbox = widgets.VBox(checkbox_rows)\n",
    "\n",
    "# Display the checkboxes in rows of 4 per line\n",
    "display(checkboxes_vbox)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scatter_2(X_1: np.array, X_2: np.array, y: np.array,\n",
    "    selected_categories: list, title: str):\n",
    "    \n",
    "    '''\n",
    "    This function will create the scatter plot of points in low dimensions.\n",
    "    It will display only the samples that belong to the `selected_category`.\n",
    "\n",
    "    Args:\n",
    "     - X_1: the first dimension of the data\n",
    "     - X_2: the second dimension of the data\n",
    "     - y: the category of the sample\n",
    "     - selected_category: the category selected by the user\n",
    "     - title: the title of the plot\n",
    "\n",
    "     Returns: nothing\n",
    "    '''\n",
    "\n",
    "    color_mapping = {cat: i for i, cat in enumerate(np.unique(y))}\n",
    "    \n",
    "    _, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    for category in np.unique(y):\n",
    "        category_mask = (y == category)\n",
    "        \n",
    "        if \"All categories\" in selected_categories:\n",
    "            ax.scatter(X_1[category_mask], X_2[category_mask], \\\n",
    "                c='C{}'.format(color_mapping[category]), label='{}'.format(category), s=8)\n",
    "        elif category in selected_categories:\n",
    "            ax.scatter(X_1[category_mask], X_2[category_mask], \\\n",
    "                c='C{}'.format(color_mapping[category]), label='{}'.format(category), s=8)\n",
    "        else:\n",
    "            ax.scatter(X_1[category_mask], X_2[category_mask], \\\n",
    "                c='gray', alpha=0.2, s=8)\n",
    "    \n",
    "    ax.set_xlabel('dim_1')\n",
    "    ax.set_ylabel('dim_2')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    sns.move_legend(ax, \"center left\", bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n",
    "\n",
    "def visualize_data_2(data: pd.DataFrame, title: str):\n",
    "    '''\n",
    "    This method is used to initiate the process of data visualization.\n",
    "    It splits the dimensionality reduced data in two arrays one for each of the dimensions\n",
    "    and turns the categories in an array as well. Then it creates and displays a dropdown list,\n",
    "    where the user can control the category displayed in the plot. The actual visualization \n",
    "    happens in the create_scatter function.\n",
    "\n",
    "    Args:\n",
    "     - data: the dimensionality reduced data\n",
    "     - title: the title of the plot that will be displayed\n",
    "\n",
    "    Returns: nothing\n",
    "    \n",
    "    '''\n",
    "\n",
    "    sns.set(\n",
    "        rc={'figure.figsize':(7, 5), 'font.family': ['DejaVu Sans']},\n",
    "        style='white'\n",
    "    )\n",
    "\n",
    "    X_1 = np.array(data[['dim_1']])\n",
    "    X_2 = np.array(data[['dim_2']])\n",
    "    y = np.array(data[['category']])\n",
    "    \n",
    "    selected_checkboxes = []\n",
    "    \n",
    "    # Function to update selected_checkboxes list\n",
    "    def update_selected_checkboxes(change):\n",
    "        checkbox = change['owner']\n",
    "        if checkbox.value:\n",
    "            selected_checkboxes.append(checkbox.description)\n",
    "        else:\n",
    "            selected_checkboxes.remove(checkbox.description)\n",
    "    \n",
    "    checkboxes = [widgets.Checkbox(value=False, description=f'{category}') for category in np.unique(y)]\n",
    "    all_checkbox = widgets.Checkbox(value=False, description='All categories')\n",
    "    checkboxes.extend([all_checkbox])\n",
    "    checkbox_rows = [widgets.HBox(checkboxes[i:i+4]) for i in range(0, 13, 4)]\n",
    "    \n",
    "    # Create a VBox layout to display the checkbox rows vertically\n",
    "    checkboxes_vbox = widgets.VBox(checkbox_rows)\n",
    "\n",
    "    # Display the checkboxes in rows of 4 per line\n",
    "    display(checkboxes_vbox)\n",
    "    \n",
    "    for checkbox in checkboxes:\n",
    "        checkbox.observe(update_selected_checkboxes, names='value')\n",
    "    \n",
    "\n",
    "    if \"All categories\" in selected_checkboxes:\n",
    "        selected_checkboxes = [\"All categories\"]\n",
    "\n",
    "    # Create a button to display selected checkboxes\n",
    "    display_button = widgets.Button(description=\"Show categories\")\n",
    "    display_button.on_click(lambda x: create_scatter_2(X_1, X_2, y, selected_checkboxes, title))\n",
    "    display(display_button)\n",
    "\n",
    "    create_scatter_2(X_1, X_2, y, [\"All categories\"], title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
